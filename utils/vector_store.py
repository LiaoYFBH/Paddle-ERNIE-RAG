import os
import logging
import random
import re
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from utils.ernie_client import ERNIEClient

# é…ç½®æ—¥å¿—
logger = logging.getLogger("vector_store")
logger.setLevel(logging.INFO)

class MilvusVectorStore:
    def __init__(self, uri, token, collection_name, embedding_service_url=None, qianfan_api_key=None):
        self.collection_name = collection_name
        self.uri = uri
        self.token = token
        self.embedding_client = ERNIEClient() 
        self._connect_milvus()
        self._init_collection()

    def _connect_milvus(self):
        try:
            if connections.has_connection("default"):
                # logger.info(f"â™»ï¸ æ£€æµ‹åˆ°å·²æœ‰è¿æ¥ï¼Œå¤ç”¨ default è¿æ¥")
                return
            
            if self.uri.endswith(".db"):
                logger.info(f"ğŸ“‚ è¿æ¥æœ¬åœ° Milvus Lite: {self.uri}")
                connections.connect("default", uri=self.uri)
            else:
                logger.info(f"ğŸŒ è¿æ¥ Milvus æœåŠ¡å™¨: {self.uri}")
                connections.connect("default", uri=self.uri, token=self.token)
        except Exception as e:
            logger.error(f"âŒ Milvus è¿æ¥å¤±è´¥: {e}")
            if not self.uri.endswith(".db") and not connections.has_connection("default"):
                # å°è¯•åˆ›å»ºä¸´æ—¶æœ¬åœ°è¿æ¥ä½œä¸ºä¿åº•
                try:
                    connections.connect("default", uri="./demo_data.db")
                except: pass

    def _init_collection(self):
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="filename", dtype=DataType.VARCHAR, max_length=256),
            FieldSchema(name="page", dtype=DataType.INT64),
            FieldSchema(name="chunk_id", dtype=DataType.INT64),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
        ]
        schema = CollectionSchema(fields, "PDF QA Collection")

        if not utility.has_collection(self.collection_name):
            self.collection = Collection(self.collection_name, schema)
            index_params = {
                "metric_type": "L2", 
                "index_type": "FLAT", 
                "params": {} 
            }
            self.collection.create_index(field_name="embedding", index_params=index_params)
            logger.info(f"âœ¨ åˆ›å»ºæ–°é›†åˆ (FLAT ç´¢å¼•): {self.collection_name}")
        else:
            self.collection = Collection(self.collection_name)
            logger.info(f"ğŸ“š åŠ è½½å·²æœ‰é›†åˆ: {self.collection_name}")
        
        self.collection.load()

    def get_embeddings(self, texts):
        if not texts: return []
        try:
            if hasattr(self.embedding_client, 'get_embeddings'):
                return self.embedding_client.get_embeddings(texts)
            else:
                return [self.embedding_client.get_embedding(t) for t in texts]
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return []

    # ğŸŒŸğŸŒŸğŸŒŸ æ ¸å¿ƒä¿®å¤: æ‰©å……åœç”¨è¯ + OR é€»è¾‘ (ä»£ç è¡Œæ•°è¡¥å…¨) ğŸŒŸğŸŒŸğŸŒŸ
    def _keyword_search(self, query, top_k=50, expr=None):
        """
        å…³é”®è¯æ£€ç´¢ (ä¿®å¤ç‰ˆï¼šä¸­è‹±æ–‡åˆ†ç»„ OR é€»è¾‘)
        """
        results = []
        try:
            # 1. å®šä¹‰æ›´å…¨é¢çš„åœç”¨è¯ (å«å¸¸è§æŒ‡ä»¤è¯)
            stop_words = {
                "çš„", "äº†", "å’Œ", "æ˜¯", "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "ç€", "æˆ–", 
                "ä¸€ä¸ª", "æ²¡æœ‰", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒ", "è§£é‡Š", "æ˜¯ä»€ä¹ˆ", 
                "å«ä¹‰", "æ–‡ç« ", "å›¾ç‰‡", "è¿™ä¸ª", "ç¯‡", "è¯·é—®", "ä»¥åŠ", "ä»€ä¹ˆ", 
                "å¦‚ä½•", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "åˆ†æ", "ä»‹ç»", "æè¿°",
                "what", "is", "the", "of", "in", "and", "to", "a", "an", "are",
                "explain", "describe", "tell", "me", "about", "how", "why", "paper", "article"
            }
            
            keywords = []
            try:
                import jieba
                # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼ï¼Œåˆ‡åˆ†æ›´ç»†
                words = jieba.cut_for_search(query) 
                for w in words:
                    w = w.strip()
                    if len(w) > 1 and w.lower() not in stop_words:
                        keywords.append(w)
            except ImportError:
                clean_query = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", " ", query)
                keywords = [w for w in clean_query.split() if w.lower() not in stop_words and len(w) > 1]

            if not keywords: return []
            
            # å»é‡
            keywords = list(set(keywords))

            # 2. å…³é”®è¯æŒ‰è¯­è¨€åˆ†ç»„
            zh_keywords = []
            en_keywords = []
            
            for k in keywords:
                # ç®€å•åˆ¤æ–­ï¼šåŒ…å«ä¸­æ–‡å³ä¸ºä¸­æ–‡å…³é”®è¯
                if any('\u4e00' <= char <= '\u9fff' for char in k):
                    zh_keywords.append(k)
                else:
                    en_keywords.append(k)
            
            # 3. æ„é€ è¡¨è¾¾å¼: ä½¿ç”¨ OR (||) é€»è¾‘ï¼Œæé«˜å¬å›ç‡
            final_parts = []
            
            # ä¸­æ–‡è¯é€šå¸¸æ¯”è¾ƒæ ¸å¿ƒï¼Œå–å‰ 5 ä¸ª
            for k in zh_keywords[:5]:
                final_parts.append(f'content like "%{k}%"')
            
            # è‹±æ–‡è¯å–å‰ 5 ä¸ª
            for k in en_keywords[:5]:
                final_parts.append(f'content like "%{k}%"')
            
            if not final_parts: return []

            # ğŸŒŸ å…³é”®ä¿®æ”¹: ç”¨ OR è¿æ¥æ‰€æœ‰æ¡ä»¶ (Logic: hit ANY keyword)
            keyword_expr = " || ".join(final_parts) 
            
            if expr:
                final_milvus_expr = f"({expr}) and ({keyword_expr})"
            else:
                final_milvus_expr = keyword_expr
            
            # 4. æ‰§è¡ŒæŸ¥è¯¢
            res = self.collection.query(
                expr=final_milvus_expr,
                output_fields=["filename", "page", "content", "chunk_id"],
                limit=top_k
            )
            
            for hit in res:
                results.append({
                    "content": hit.get("content"),
                    "filename": hit.get("filename"),
                    "page": hit.get("page"),
                    "chunk_id": hit.get("chunk_id"),
                    "semantic_score": 0.0,  
                    "raw_score": 100.0,     
                    "type": "keyword",
                    "id": hit.get("id")
                })
        except Exception as e:
            print(f"âš ï¸ å…³é”®è¯æ£€ç´¢è·³è¿‡: {e}")
            
        return results

    def search(self, query: str, top_k: int = 10, **kwargs):
        """
        ã€é«˜ç²¾åº¦æ··åˆæ£€ç´¢ã€‘
        """
        expr = kwargs.get('expr', None)

        # === 1. å‘é‡æ£€ç´¢ (Dense) ===
        dense_results = []
        try:
            query_vector = self.embedding_client.get_embedding(query)
            if query_vector:
                search_params = {"metric_type": "L2", "params": {}} 
                
                milvus_res = self.collection.search(
                    data=[query_vector],
                    anns_field="embedding", 
                    param=search_params,
                    limit=top_k * 5,
                    expr=expr, 
                    output_fields=["filename", "page", "content", "chunk_id"]
                )
                
                for hit in milvus_res[0]:
                    raw_score = 1.0 / (1.0 + hit.distance) * 100
                    dense_results.append({
                        "content": hit.entity.get("content"),
                        "filename": hit.entity.get("filename"),
                        "page": hit.entity.get("page"),
                        "chunk_id": hit.entity.get("chunk_id"),
                        "semantic_score": hit.distance, 
                        "raw_score": raw_score,
                        "type": "dense",
                        "id": hit.id
                    })
        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢å¼‚å¸¸: {e}")

        # === 2. å…³é”®è¯æ£€ç´¢ (Keyword) ===
        keyword_results = self._keyword_search(query, top_k=top_k * 5, expr=expr)

        # === 3. RRF èåˆ ===
        rank_dict = {}
        
        def apply_rrf(results_list, k=60, weight=1.0):
            for rank, item in enumerate(results_list):
                doc_id = item.get('id') or item.get('chunk_id')
                if doc_id not in rank_dict:
                    rank_dict[doc_id] = {"data": item, "score": 0.0}
                rank_dict[doc_id]["score"] += weight * (1.0 / (k + rank))

        apply_rrf(dense_results, weight=1.0)
        apply_rrf(keyword_results, weight=3.0) 

        # === 4. æ’åºè¾“å‡º ===
        sorted_docs = sorted(rank_dict.values(), key=lambda x: x['score'], reverse=True)
        final_results = [item['data'] for item in sorted_docs[:top_k * 2]]
        
        print(f"ğŸ” æ··åˆæ£€ç´¢: å‘é‡{len(dense_results)} + å…³é”®è¯{len(keyword_results)} -> èåˆ{len(final_results)}")
        return final_results

    
    def insert_documents(self, documents):
        if not documents: return
        print(f"âš¡ æ­£åœ¨è¯·æ±‚ Embedding (å…± {len(documents)} æ¡)...")
        texts = [doc['content'] for doc in documents]
        
        embeddings = self.get_embeddings(texts)

        valid_docs, valid_vectors = [], []
        failed_count = 0
        
        for i, emb in enumerate(embeddings):
            if emb and len(emb) == 384:
                valid_docs.append(documents[i])
                valid_vectors.append(emb)
            else:
                failed_count += 1
        
        if failed_count > 0:
            print(f"âš ï¸ è­¦å‘Š: æœ‰ {failed_count} æ¡ç‰‡æ®µ Embedding å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œæˆ–Keyé—®é¢˜ï¼‰")
            
        if not valid_docs: 
            print("âŒ ä¸¥é‡é”™è¯¯: æ‰€æœ‰ç‰‡æ®µ Embedding å‡å¤±è´¥ï¼Œæ•°æ®æœªå…¥åº“ï¼")
            print("ğŸ‘‰ è¯·æ£€æŸ¥: 1. AISTUDIO_ACCESS_TOKEN æ˜¯å¦æ­£ç¡®/è¿‡æœŸ") # ğŸŒŸ æ¢å¤äº†è¿™è¡Œ
            print("ğŸ‘‰ è¯·æ£€æŸ¥: 2. ç½‘ç»œæ˜¯å¦é€šç•…")                       # ğŸŒŸ æ¢å¤äº†è¿™è¡Œ
            return

        try:
            data = [
                [doc['filename'] for doc in valid_docs],
                [doc['page'] for doc in valid_docs],
                [doc['chunk_id'] for doc in valid_docs],
                [doc['content'] for doc in valid_docs],
                valid_vectors
            ]
            self.collection.insert(data)
            self.collection.flush() # å¼ºåˆ¶åˆ·ç›˜
            logger.info(f"âœ… æˆåŠŸå…¥åº“: å·²æ’å…¥ {len(valid_vectors)} æ¡æ•°æ®")
        except Exception as e:
            print(f"âŒ Milvus å†™å…¥å¼‚å¸¸: {e}")

    def delete_document(self, filename):
        """
        æ ¹æ®æ–‡ä»¶ååˆ é™¤å‘é‡æ•°æ®
        """
        if not filename: return "âŒ æ–‡ä»¶åä¸ºç©º"
        try:
            # 1. æ‰§è¡Œåˆ é™¤ (ä½¿ç”¨ expr è¡¨è¾¾å¼)
            self.collection.delete(expr=f'filename == "{filename}"')
            
            # 2. å¼ºåˆ¶åˆ·ç›˜ (è®©åˆ é™¤ç«‹å³ç”Ÿæ•ˆ)
            self.collection.flush()
            
            logger.info(f"ğŸ—‘ï¸ å·²ä»åº“ä¸­åˆ é™¤æ–‡æ¡£: {filename}")
            return f"âœ… å·²æˆåŠŸåˆ é™¤: {filename}"
        except Exception as e:
            err_msg = f"âŒ åˆ é™¤å¤±è´¥: {e}"
            logger.error(err_msg)
            return err_msg

    def list_documents(self):
        try:
            # é™åˆ¶æŸ¥è¯¢æ•°é‡
            res = self.collection.query(expr="id > 0", output_fields=["filename"], limit=16384)
            return sorted(list(set([r['filename'] for r in res])))
        except: return []

    def get_document_content(self, filename):
        try:
            res = self.collection.query(expr=f"filename == '{filename}'", output_fields=["content", "page"], limit=1000)
            res.sort(key=lambda x: x['page'])
            return "\n\n".join([r['content'] for r in res])
        except: return ""

    def test_self_recall(self, sample_size=20):
        """
        è‡ªå›å½’å¬å›æµ‹è¯•
        """
        try:
            total = self.collection.num_entities
            if total == 0: return "âŒ åº“ä¸ºç©ºï¼Œæ— æ³•æµ‹è¯•"

            limit = min(100, total)
            res = self.collection.query(expr="id > 0", output_fields=["id", "content"], limit=limit)
            
            if not res: return "âŒ æ— æ³•è·å–æ•°æ®"
            
            samples = random.sample(res, min(sample_size, len(res)))
            
            hits = 0
            for item in samples:
                doc_id = item['id']
                content = item['content']
                emb = self.embedding_client.get_embedding(content)
                if not emb: continue
                
                search_res = self.collection.search(
                    data=[emb], 
                    anns_field="embedding", 
                    param={"metric_type": "L2", "params": {}}, 
                    limit=1,
                    output_fields=["id"]
                )
                
                if search_res and len(search_res[0]) > 0:
                    top1_id = search_res[0][0].id
                    if top1_id == doc_id:
                        hits += 1
            
            recall_rate = (hits / len(samples)) * 100
            return f"âœ… å¬å›æµ‹è¯• ({len(samples)}æ¡æ ·æœ¬): å‡†ç¡®ç‡ {recall_rate:.1f}%"
            
        except Exception as e:
            return f"âŒ æµ‹è¯•å‡ºé”™: {e}"