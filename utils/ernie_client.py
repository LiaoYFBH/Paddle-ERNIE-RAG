import os
from openai import OpenAI
import time 
import logging
import random
import requests
import json
import base64
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ernie_client")

class ERNIEClient:
    """
    ç™¾åº¦ ERNIE / OpenAI å…¼å®¹å®¢æˆ·ç«¯
    å·²æ›´æ–°ï¼šé’ˆå¯¹åƒå¸†åŸç”Ÿ API çš„ 429 é™æµåšæ·±åº¦ä¼˜åŒ–
    """
    
    def __init__(self, 
                 llm_api_base=None, llm_api_key=None, llm_model=None,
                 embed_api_base=None, embed_api_key=None, embed_model=None,
                 qps=0.8): # ğŸŒŸ é»˜è®¤ QPS è°ƒä½è‡³ 0.8ï¼Œæ›´å®‰å…¨
        
        # === 1. LLM é…ç½® ===
        self.llm_base = (llm_api_base or "https://aistudio.baidu.com/llm/lmapi/v3").rstrip('/')
        self.llm_key = llm_api_key or os.getenv("AISTUDIO_ACCESS_TOKEN", "")
        self.chat_model_name = llm_model or "ernie-4.5-turbo-vl"#"ernie-4.5-turbo-128k-preview"
        
        # === 2. Embedding é…ç½® ===
        self.embed_base = (embed_api_base or "https://aistudio.baidu.com/llm/lmapi/v3").rstrip('/')
        self.embed_key = embed_api_key or os.getenv("AISTUDIO_ACCESS_TOKEN", "")
        self.embedding_model_name = embed_model or "embedding-v1"

        # === 4. é€Ÿç‡æ§åˆ¶ ===
        self.target_qps = float(qps) if qps > 0 else 0.8
        self.current_delay = 1.0 / self.target_qps 
        
        self.last_embed_time = 0
        self.last_chat_time = 0
        self.max_retries = 5 # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # === 5. åˆå§‹åŒ–å®¢æˆ·ç«¯ ===
        self.chat_client = None
        self.embed_client = None
        self._init_clients()

    def _init_clients(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ä»…å½“ä¸æ˜¯åƒå¸†åŸç”Ÿæ¨¡å¼æ—¶)"""
        if self.llm_key:
            try:
                self.chat_client = OpenAI(base_url=self.llm_base, api_key=self.llm_key, max_retries=self.max_retries, timeout=120.0)
            except Exception as e: logger.error(f"âŒ LLM Client åˆå§‹åŒ–å¼‚å¸¸: {e}")

        if self.embed_key:
            try:
                self.embed_client = OpenAI(base_url=self.embed_base, api_key=self.embed_key, max_retries=self.max_retries, timeout=120.0)
            except Exception as e: logger.error(f"âŒ Embedding Client åˆå§‹åŒ–å¼‚å¸¸: {e}")
    def _encode_image(self, image_path):
            """è¾…åŠ©ï¼šè¯»å–å›¾ç‰‡å¹¶è½¬ Base64"""
            try:
                with open(image_path, "rb") as image_file:
                    return base64.b64encode(image_file.read()).decode('utf-8')
            except Exception as e:
                # å¦‚æœè¯»å›¾å¤±è´¥ï¼Œæ‰“æ—¥å¿—ï¼Œè¿”å› None
                print(f"âŒ å›¾ç‰‡è¯»å–/ç¼–ç å¤±è´¥: {e}") 
                return None

    def chat_with_image(self, query: str, image_path: str):
        """
        å‘é€å¸¦å›¾ç‰‡çš„å¯¹è¯è¯·æ±‚ (Vision)
        """
        base64_image = self._encode_image(image_path)
        
        # 1. ç¼–ç å¤±è´¥ï¼Œé™çº§
        if not base64_image:
            print("âš ï¸ å›¾ç‰‡ç¼–ç å¤±è´¥ï¼Œé™çº§ä¸ºçº¯æ–‡æœ¬é—®ç­”")
            return self.chat([{"role": "user", "content": query}])
        
        # 2. æ„é€  Vision æ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": query},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ]
        
        # 3. å°è¯•å‘é€ï¼Œå¹¶æ•è·ç‰¹å®šé”™è¯¯
        try:
            return self.chat(messages)
        except Exception as e:
            # æŠ›å‡ºå¼‚å¸¸ä¾›ä¸Šå±‚ (backend.py) æ•è·å’Œå¤„ç†
            raise e
    def _wait_for_rate_limit(self, is_embedding=True):
        """æµæ§ç­‰å¾…"""
        now = time.time()
        last_time = self.last_embed_time if is_embedding else self.last_chat_time
        elapsed = now - last_time
        if elapsed < self.current_delay:
            time.sleep(self.current_delay - elapsed)
        
        # æ›´æ–°æ—¶é—´æˆ³
        if is_embedding: self.last_embed_time = time.time()
        else: self.last_chat_time = time.time()

    def _adaptive_slow_down(self):
        """è§¦å‘è‡ªé€‚åº”é™çº§ï¼šé‡åˆ°é™æµæ—¶ï¼Œæ°¸ä¹…å¢åŠ é—´éš”"""
        self.current_delay = min(self.current_delay * 2.0, 15.0) 
        logger.warning(f"ğŸ“‰ è§¦å‘é€Ÿç‡é™åˆ¶(429)ï¼Œç³»ç»Ÿè‡ªåŠ¨é™é€Ÿ: æ–°é—´éš” {self.current_delay:.2f}s")

    def chat(self, messages: list, model=None, max_tokens=2048, temperature=0.7):
        use_model = model if model else self.chat_model_name
        self._wait_for_rate_limit(is_embedding=False)

        if not self.chat_client: return "é”™è¯¯: Client æœªåˆå§‹åŒ–"
        
        try:
            response = self.chat_client.chat.completions.create(
                model=use_model, messages=messages, max_tokens=max_tokens, temperature=temperature
            )
            self.last_chat_time = time.time()
            content = response.choices[0].message.content
            if not content: return "æ¨¡å‹è¿”å›å†…å®¹ä¸ºç©º"
            return content
            
        except Exception as e:
            # ğŸ›‘ å…³é”®ï¼šä¸è¦åœ¨è¿™é‡Œåªæ‰“å°æ—¥å¿—ç„¶åè¿”å› None/Str
            # æˆ‘ä»¬éœ€è¦æŠŠåŸå§‹é”™è¯¯ raise å‡ºå»ï¼Œæˆ–è€…è¿”å›ä¸€ä¸ªå¸¦æœ‰ç‰¹æ®Šæ ‡è®°çš„é”™è¯¯å¯¹è±¡
            # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬è¿™é‡Œ raiseï¼Œè®© backend å» try-catch
            logger.error(f"âŒ Chat å¤±è´¥: {e}")
            raise e

    def get_embedding(self, text: str, max_retries: int = 5) -> list:
        if not text: return None
            
        for attempt in range(max_retries):
            try:
                self._wait_for_rate_limit(is_embedding=True)
                
                # === åˆ†æ”¯ B: OpenAI å…¼å®¹æ¨¡å¼ ===
                if self.embed_client:
                    response = self.embed_client.embeddings.create(
                        model=self.embedding_model_name, input=[text]
                    )
                    self.last_embed_time = time.time()
                    if response and response.data:
                        return response.data[0].embedding

            except Exception as e:
                error_str = str(e).lower()
                
                # ğŸŒŸ æ ¸å¿ƒé€»è¾‘ï¼šè¯†åˆ«åƒå¸†ç‰¹å®šçš„é™æµé”™è¯¯ç 
                is_rate_limit = (
                    "429" in error_str or 
                    "rate limit" in error_str or 
                    "rpm_rate_limit_exceeded" in error_str or
                    "tpm_rate_limit_exceeded" in error_str
                )
                
                if is_rate_limit:
                    self._adaptive_slow_down() # æ°¸ä¹…é™é€Ÿ
                    
                    # æœ¬æ¬¡é¿è®© (æŒ‡æ•°é€€é¿)
                    wait_time = (2 ** attempt) + random.uniform(1.0, 3.0)
                    logger.warning(f"âš ï¸ è§¦å‘é™æµä¿æŠ¤ï¼Œé¿è®© {wait_time:.1f}s (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"âš ï¸ Embedding å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
                    time.sleep(1)
        
        logger.error("âŒ Embedding æœ€ç»ˆå¤±è´¥")
        return None

    def get_embeddings(self, texts: list) -> list:
        """æ‰¹é‡è·å–"""
        if not texts: return []
        results = []
        for t in texts:
            emb = self.get_embedding(t)
            results.append(emb)
        return results
    
    get_embeddings_batch = get_embeddings

    def answer_question(self, question: str, context_chunks: list) -> str:
        if not context_chunks:
            prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{question}"
        else:
            context_str = ""
            for i, chunk in enumerate(context_chunks):
                content = chunk.get('content', '').replace('\n', ' ')[:800]
                fname = chunk.get('filename', 'æœªçŸ¥æ–‡æ¡£')
                page = chunk.get('page', 0)
                context_str += f"[å‚è€ƒèµ„æ–™{i+1} ({fname} P{page})]: {content}\n\n"
            prompt = f"åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š\n\n[å‚è€ƒèµ„æ–™]:\n{context_str}\n\n[ç”¨æˆ·é—®é¢˜]:\n{question}"
        return self.chat([{"role": "user", "content": prompt}]) or "ç”Ÿæˆå›ç­”å¤±è´¥"

    def generate_summary(self, text: str) -> str:
        if not text: return "æ— å†…å®¹"
        prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆä¸€ä»½ç²¾ç®€æ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰ï¼š\n\n{text[:5000]}"
        return self.chat([{"role": "user", "content": prompt}]) or "æ‘˜è¦ç”Ÿæˆå¤±è´¥"

    def rewrite_query(self, query: str) -> str:
        prompt = f"""è¯·å°†ä»¥ä¸‹æœç´¢æŸ¥è¯¢é‡å†™ä¸ºä¸€ä¸ªæ›´è¯¦ç»†ã€åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡å…³é”®è¯çš„é™ˆè¿°å¥ï¼Œä»¥ä¾¿äºå‘é‡æ£€ç´¢ã€‚
        
        åŸå§‹æŸ¥è¯¢: "{query}"
        é‡å†™å:"""
        res = self.chat([{"role": "user", "content": prompt}], max_tokens=200)
        return res if res else query