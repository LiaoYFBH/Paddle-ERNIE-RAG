import os
import sys

os.environ["FLAGS_enable_pir_api"] = "0"           # ç¦ç”¨ PIR 
os.environ["FLAGS_use_cin_compiler"] = "0"         # ç¦ç”¨ CIN
os.environ["FLAGS_allocator_strategy"] = "auto_growth" 
os.environ["CUDA_VISIBLE_DEVICES"] = ""            # å¼ºåˆ¶ CPU
# === ğŸ›‘ [æ–°å¢] å¼ºåŠ›ä¿®å¤æ­¥éª¤ ===
import paddle  # æ˜¾å¼å¯¼å…¥ paddle
try:
    paddle.set_device("cpu")  # ğŸ”’ å¼ºåˆ¶é”å®šå…¨å±€è®¾å¤‡ä¸º CPU
    print("ğŸ”’ å·²å¼ºåˆ¶è®¾ç½® paddle.set_device('cpu')")
except Exception as e:
    print(f"âš ï¸ è­¦å‘Š: å¼ºåˆ¶è®¾ç½® CPU å¤±è´¥: {e}")
# ============================
import subprocess
import yaml
import logging
import shutil
import base64
import time
import socket
from pathlib import Path
import gradio as gr
from dotenv import load_dotenv
from PIL import Image
import numpy as np
import io

try:
    from paddlex import create_pipeline
    print("âœ… PaddleX å¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âŒ æœªå®‰è£… paddlexï¼Œè¯·è¿è¡Œ: pip install paddlex")
    exit(1)
from pymilvus import utility, connections

from utils.vector_store import MilvusVectorStore
from utils.ernie_client import ERNIEClient
from utils.reranker_v2 import RerankerAndFilterV2

load_dotenv()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
silence_libs = ["httpx", "httpcore", "urllib3", "asyncio", "pdf_qa", "gradio", "multipart", "PIL", "matplotlib", "ppocr", "paddle"]
for lib in silence_libs:
    logging.getLogger(lib).setLevel(logging.ERROR)

ASSET_DIR = "assets"
os.makedirs(ASSET_DIR, exist_ok=True)
CONFIG_DIR = "my_configs"
os.makedirs(CONFIG_DIR, exist_ok=True)

print("âš™ï¸  ç³»ç»Ÿå¯åŠ¨ä¸­...")
pipeline_engine = None 
current_pipeline_lang = None  

def get_optimized_config_path(lang="ch"):
    """
    ç”Ÿæˆé’ˆå¯¹ CPU ä¼˜åŒ–çš„è½»é‡çº§æ¨¡å‹é…ç½® (æ”¯æŒä¸­è‹±æ–‡åˆ‡æ¢)
    lang: "ch" (é€šç”¨) æˆ– "en" (çº¯è‹±æ–‡)
    """
    original_config_name = "PP-StructureV3.yaml"
    target_config_name = f"lightweight_structure_v3_{lang}.yaml"
    target_path = os.path.abspath(os.path.join(CONFIG_DIR, target_config_name))

    if not os.path.exists(os.path.join(CONFIG_DIR, original_config_name)):
        try:
            subprocess.run(
                ["paddlex", "--get_pipeline_config", "PP-StructureV3", "--save_path", CONFIG_DIR],
                check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            )
        except: pass

    # è¯»å–é…ç½®
    config_data = None
    src_yaml = os.path.join(CONFIG_DIR, original_config_name)
    if os.path.exists(src_yaml):
        try:
            with open(src_yaml, 'r', encoding='utf-8') as f: config_data = yaml.safe_load(f)
        except: pass
        
    # ä¿åº•é…ç½®
    if not config_data:
        config_data = {
            "pipeline_name": "PP-StructureV3",
            "SubModules": {
                "LayoutDetection": {"module_name": "layout_detection", "model_name": "PP-DocLayout-S", "batch_size": 1}
            },
            "SubPipelines": {
                "DocPreprocessor": {
                    "pipeline_name": "DocPreprocessor",
                    "SubModules": {
                        "DocOrientationClassify": {"module_name": "doc_orientation_classification", "model_name": "PP-LCNet_x1_0_doc_ori"},
                        "DocUnwarping": {"module_name": "doc_unwarping", "model_name": "UVDoc"}
                    }
                },
                "GeneralOCR": {
                    "pipeline_name": "OCR",
                    "text_type": "general",
                    "SubModules": {
                        "TextDetection": {"module_name": "text_detection", "model_name": "PP-OCRv4_mobile_det", "limit_side_len": 736},
                        "TextRecognition": {"module_name": "text_recognition", "model_name": "PP-OCRv4_mobile_rec"}
                    }
                },
                "TableRecognition": {
                    "pipeline_name": "TableRecognition",
                    "SubModules": {
                        "TableStructureRecognition": {"module_name": "table_structure_recognition", "model_name": "SLANeXt_wired"},
                        "TableClassification": {"module_name": "table_classification", "model_name": "PP-LCNet_x1_0_table_cls"}
                    }
                }
            }
        }
    def recursive_replace(d):
        if isinstance(d, dict):
            for k, v in d.items():
                if k == "model_name" and isinstance(v, str):
                    if "PP-DocLayout" in v: 
                        d[k] = "PP-DocLayout-S"
                    elif "PP-OCRv4" in v:
                        if "det" in v:
                            d[k] = "PP-OCRv4_mobile_det"
                        elif "rec" in v:
                            if lang == "en":
                                # çº¯è‹±æ–‡æ¨¡å¼ï¼šä½¿ç”¨è‹±æ–‡ä¸“ç”¨è¯†åˆ«æ¨¡å‹
                                d[k] = "en_PP-OCRv4_mobile_rec"
                            else:
                                # é€šç”¨æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤ä¸­è‹±æ–‡æ¨¡å‹
                                d[k] = "PP-OCRv4_mobile_rec"
                    elif "PP-FormulaNet" in v: 
                        d[k] = "PP-FormulaNet-S"
                    elif "seal" in v:
                        if "det" in v: d[k] = "PP-OCRv4_mobile_seal_det"
                
                elif k == "limit_side_len" and isinstance(v, int) and v > 736:
                    d[k] = 736
                else:
                    recursive_replace(v)
        elif isinstance(d, list):
            for item in d: recursive_replace(item)
    
    recursive_replace(config_data)

    # ä¿å­˜æœ€ç»ˆé…ç½®
    with open(target_path, 'w', encoding='utf-8') as f:
        yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ… å·²ç”Ÿæˆ {lang} æ¨¡å¼é…ç½®: {target_path}")
    return target_path
def get_paddlex_pipeline(lang="ch"):
    """åŠ è½½äº§çº¿ (æ£€æµ‹è¯­è¨€å˜æ›´)"""
    global pipeline_engine, current_pipeline_lang
    
    if pipeline_engine is not None and current_pipeline_lang == lang:
        return pipeline_engine

    print(f"â³ æ­£åœ¨åˆå§‹åŒ– PaddleX ({lang} æ¨¡å¼)...")
    
    
    # è·å–å¯¹åº”è¯­è¨€çš„é…ç½®æ–‡ä»¶
    config_path = get_optimized_config_path(lang)
    
    try:
        pipeline_engine = None 
        
        pipeline_engine = create_pipeline(
            pipeline=config_path, 
            device="cpu",  
        )
        current_pipeline_lang = lang 
        print(f"ğŸš€ PaddleX å¼•æ“åŠ è½½æˆåŠŸï¼(æ¨¡å¼: {lang})")
    except Exception as e:
        print(f"âŒ å¼•æ“åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
            
    return pipeline_engine
ernie = None
milvus_store = None
reranker_filter = None
known_collections = {}
system_ready = False

def check_ready():
    if not system_ready: return False, "âš ï¸ ç³»ç»Ÿæœªè¿æ¥"
    return True, ""

def initialize_system(aistudio_token, qianfan_key, milvus_uri, milvus_token):
    global ernie, milvus_store, reranker_filter, system_ready, known_collections

    aistudio_token = aistudio_token.strip() if aistudio_token else ""
    qianfan_key = qianfan_key.strip() if qianfan_key else ""
    milvus_uri = milvus_uri.strip() if milvus_uri else ""
    milvus_token = milvus_token.strip() if milvus_token else ""

    # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶æ¨¡å¼ (.db)ï¼Œåˆ™ä¸éœ€è¦æ ¡éªŒ milvus_token
    is_local_mode = milvus_uri.endswith(".db")
    
    # å¿…å¡«é¡¹æ£€æŸ¥ï¼šAIStudioã€åƒå¸†ã€URI å¿…é¡»æœ‰
    basic_check = all([aistudio_token, qianfan_key, milvus_uri])
    # Tokenæ£€æŸ¥ï¼šå¦‚æœæ˜¯æœåŠ¡å™¨æ¨¡å¼ï¼Œåˆ™å¿…é¡»æœ‰ Tokenï¼›å¦‚æœæ˜¯æœ¬åœ°æ¨¡å¼ï¼ŒToken å¯ä¸ºç©º
    token_check = True if is_local_mode else bool(milvus_token)

    if not (basic_check and token_check):
        return "âŒ è¯·å¡«å†™å¿…è¦ä¿¡æ¯ (æœ¬åœ°æ¨¡å¼æ— éœ€ Tokenï¼Œä½†åœ¨æœåŠ¡å™¨æ¨¡å¼ä¸‹å¿…å¡«)", gr.update(), gr.update(), gr.update()

    try:
        os.environ["AISTUDIO_ACCESS_TOKEN"] = aistudio_token
        os.environ["QIANFAN_API_KEY"] = qianfan_key
        os.environ["MILVUS_URI"] = milvus_uri
        if milvus_token:
            os.environ["MILVUS_TOKEN"] = milvus_token
        else:
            os.environ.pop("MILVUS_TOKEN", None)

        ernie = ERNIEClient()
        reranker_filter = RerankerAndFilterV2()

        milvus_store = MilvusVectorStore(
            uri=milvus_uri,
            token=milvus_token, 
            collection_name="pdf_qa_collection_paddle_v3", 
            embedding_service_url="https://aistudio.baidu.com/llm/lmapi/v3",
            qianfan_api_key=aistudio_token
        )
        
        known_collections = {milvus_store.collection_name: milvus_store}
        try:
            scan_remote_collections()
        except: pass
        
        cols = list(known_collections.keys())
        default_col = cols[0] if cols else None
        
        system_ready = True
        get_paddlex_pipeline()
        return (
            "âœ… è¿æ¥æˆåŠŸ", 
            gr.update(choices=cols, value=default_col), 
            gr.update(choices=cols, value=default_col), 
            gr.update(choices=cols, value=default_col)
        )
        
    except Exception as e:
        return f"âŒ å¤±è´¥: {str(e)}", gr.update(), gr.update(), gr.update()

def scan_remote_collections():
    global known_collections
    try:
        alias = f"scan_{int(time.time())}"
        connections.connect(alias=alias, uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"))
        all_colls = utility.list_collections(using=alias)
        connections.disconnect(alias)
        for name in all_colls:
            if name not in known_collections:
                known_collections[name] = MilvusVectorStore(
                    uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"),
                    collection_name=name, embedding_service_url="https://aistudio.baidu.com/llm/lmapi/v3",
                    qianfan_api_key=os.environ.get("AISTUDIO_ACCESS_TOKEN")
                )
        return list(known_collections.keys())
    except:
        return list(known_collections.keys())

def split_text_into_chunks(text: str, chunk_size: int = 350, overlap: int = 100) -> list:
    """
    æ–‡æœ¬åˆ‡åˆ† (ä¿®å¤ç‰ˆ)ï¼šå¼ºåˆ¶åˆ‡åˆ†è¶…é•¿è¡Œï¼Œé˜²æ­¢ API æŠ¥é”™
    """
    if not text: return []
    
    lines = [line.strip() for line in text.split("\n") if line.strip()]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for line in lines:
        while len(line) > chunk_size:
            # æˆªå–ä¸€æ®µ
            part = line[:chunk_size]
            # å‰©ä¸‹çš„æ”¾å›å»ç»§ç»­å¾ªç¯å¤„ç†
            line = line[chunk_size:]
            
            if current_length + len(part) > chunk_size and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = []
                current_length = 0
            
            current_chunk.append(part)
            current_length += len(part)
            
            chunks.append("\n".join(current_chunk))
            current_chunk = []
            current_length = 0
        
        if current_length + len(line) > chunk_size and current_chunk:
            chunks.append("\n".join(current_chunk))
        
            overlap_text = current_chunk[-1][-overlap:] if current_chunk else ""
            current_chunk = [overlap_text] if overlap_text else []
            current_length = len(overlap_text)
            
        current_chunk.append(line)
        current_length += len(line)
        
    # å¤„ç†æœ€åçš„å°¾å·´
    if current_chunk:
        content = "\n".join(current_chunk).strip()
        if content: chunks.append(content)
        
    return chunks
def process_uploaded_pdf(files, collection_name, ocr_lang_choice, progress=gr.Progress()):
    lang_code = "en" if "English" in ocr_lang_choice else "ch"
    if collection_name: collection_name = str(collection_name).strip()
    
    ready, msg = check_ready()
    if not ready: return msg
    if not files: return "âš ï¸ è¯·ä¸Šä¼  PDF"
    
    if collection_name not in known_collections:
        create_collection_ui(collection_name)
    
    target_store = known_collections[collection_name]
    results = [] 
    col_img_dir = os.path.join(ASSET_DIR, collection_name)
    try: os.makedirs(col_img_dir, exist_ok=True)
    except: pass
    
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ {collection_name} ä¸­çš„æ–‡æ¡£åˆ—è¡¨...")
    try:
        existing_files = set(target_store.list_documents())
    except Exception as e:
        print(f"âš ï¸ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        existing_files = set()

    print(f"\n[ç³»ç»Ÿ] æ­£åœ¨è·å– PaddleX å¼•æ“ (è¯­è¨€: {lang_code})...")
    engine = get_paddlex_pipeline(lang=lang_code)
    
    if engine is None:
        return "âŒ å†…éƒ¨é”™è¯¯: AI å¼•æ“åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°æŠ¥é”™"

    for file_path in progress.tqdm(files, desc="PaddleX æ‰¹é‡è§£æä¸­"):
        
       
        path_str = file_path.name if hasattr(file_path, 'name') else file_path
        filename = os.path.basename(path_str)
        abs_path = os.path.abspath(path_str)
    
        if filename in existing_files:
            log_msg = f"â© {filename} (å·²å­˜åœ¨ï¼Œè·³è¿‡)"
            print(log_msg)
            results.append(log_msg) # æ·»åŠ åˆ°UIæ—¥å¿—
            time.sleep(0.05) # ç»™UIçº¿ç¨‹ä¸€ç‚¹åˆ·æ–°æ—¶é—´
            continue
        
        file_img_dir = os.path.join(col_img_dir, os.path.splitext(filename)[0])
        if os.path.exists(file_img_dir): shutil.rmtree(file_img_dir)
        os.makedirs(file_img_dir, exist_ok=True)
        
        print(f"\nğŸš€ å¼€å§‹è§£ææ–‡ä»¶: {filename}")
        
        # è°ƒç”¨ PaddleX é¢„æµ‹
        try:
            prediction = engine.predict(
                input=abs_path, 
                device="cpu", 
                use_doc_orientation_classify=False,
                use_doc_unwarping=False,
                use_textline_orientation=False
            )
            output = list(prediction)
        except Exception as e:
            err_msg = f"âŒ {filename}: å¼•æ“è§£æå¼‚å¸¸ ({str(e)})"
            print(err_msg)
            results.append(err_msg)
            continue

        file_chunk_count = 0 
        if output:
            for page_idx, res in enumerate(output):
                if not (hasattr(res, 'markdown') and res.markdown):
                    continue
                    
                md_data = res.markdown
                page_text = md_data.get('markdown_texts', '') 
                page_images = md_data.get('markdown_images', {})
             
                for img_path, img_val in page_images.items():
                    try:
                        sname = f"p{page_idx}_{int(time.time())}_{os.path.basename(img_path)}"
                        spath = os.path.join(file_img_dir, sname)
                        if isinstance(img_val, str):
                            with open(spath, "wb") as f: f.write(base64.b64decode(img_val))
                        elif hasattr(img_val, 'save'):
                            img_val.save(spath)
                        # æ›¿æ¢æ–‡æœ¬ä¸­çš„å›¾ç‰‡è·¯å¾„
                        page_text = page_text.replace(img_path, f"[å›¾è¡¨: {sname}]")
                    except: pass
                
                if not page_text.strip(): continue

                page_chunks = split_text_into_chunks(page_text)
                
                docs = []
                for cid, chunk in enumerate(page_chunks):
                    docs.append({
                        "filename": filename, 
                        "page": page_idx,  # å†™å…¥çœŸå®é¡µç 
                        "content": f"æ–‡æ¡£: {filename} (P{page_idx+1})\n{chunk}", 
                        "chunk_id": file_chunk_count + cid
                    })
                
                if docs:
                    target_store.insert_documents(docs)
                    file_chunk_count += len(docs)
                    # print(f"  -> P{page_idx+1} å…¥åº“ {len(docs)} æ¡")

        if file_chunk_count > 0:
            success_msg = f"âœ… {filename} (æå– {file_chunk_count} ç‰‡æ®µ)"
            results.append(success_msg)
        else:
            fail_msg = f"âŒ {filename}: æœªæå–åˆ°æœ‰æ•ˆå†…å®¹"
            results.append(fail_msg)
            
        time.sleep(0.05)
            
    return "\n".join(results)

def ask_question_logic(question, collection_name, target_filename=None):
    ready, msg = check_ready()
    if not ready: return msg, "N/A"
    if not question.strip(): return "è¯·è¾“å…¥é—®é¢˜", "0.0%"
    
    target_store = known_collections.get(collection_name, milvus_store)
    search_kwargs = {"top_k": 20}
    if target_filename and target_filename != "å…¨éƒ¨æ–‡æ¡£ (Global QA)":
        search_kwargs["expr"] = f"filename == '{target_filename}'"
        
    retrieved = target_store.search(question, **search_kwargs)
    if not retrieved: return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚", "0.0%"
    
    processed, _ = reranker_filter.process(question, retrieved)
    final = processed[:5]
    top_score = final[0].get('composite_score', 0) if final else 0
    metric = f"{min(100, top_score/1.2):.1f}%"
    
    answer = ernie.answer_question(question, final)
    seen = set()
    sources = "\n\nğŸ“š **å‚è€ƒæ¥æº:**\n"
    for c in final:
    
        page_num = c.get('page', 0) + 1
        fname = c.get('filename', 'æœªçŸ¥æ–‡æ¡£')

        key = f"{fname} (P{page_num})"
        
        if key not in seen:
            # æ˜¾ç¤ºæ ¼å¼: - æ–‡ä»¶å (Pé¡µç ) [Rel:åˆ†æ•°]
            sources += f"- {key} [Rel:{c.get('composite_score',0):.0f}]\n"
            seen.add(key)
    return answer + sources, metric

def handle_image_upload(file, history):
    if not file: return history, ""
    history.append({"role": "user", "content": (file.name,)})
    try:
        engine = get_paddlex_pipeline()
        output = engine.predict(input=file.name)
        extracted_text = ""
        for res in output:
            if hasattr(res, 'markdown'): extracted_text += res.markdown.get('text', '') + "\n"

        if extracted_text:
            history.append({"role": "assistant", "content": f"âœ… å†…å®¹:\n{extracted_text[:300]}..."})
        else:
            history.append({"role": "assistant", "content": "âš ï¸ æœªè¯†åˆ«åˆ°å†…å®¹ã€‚"})
    except Exception as e:
        history.append({"role": "assistant", "content": f"âŒ å¤±è´¥: {e}"})
    return history, extracted_text

def chat_respond(message, history, collection_name, target_filename, img_context):
    if not message: return history, history, "", "N/A", img_context
    if not collection_name: 
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "âš ï¸ è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“"})
        return history, history, "", "N/A", img_context

    full_query = message
    if img_context: 
        full_query = f"{img_context}\nç”¨æˆ·é—®é¢˜: {message}"
        img_context = "" 
    
    answer, metric = ask_question_logic(full_query, collection_name, target_filename)
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": answer})
    return history, history, "", metric, img_context

def analyze_doc_and_images(collection_name, filename):
    ready, msg = check_ready()
    if not ready: return "ç³»ç»Ÿæœªè¿æ¥", []
    if not filename or filename == "å…¨éƒ¨æ–‡æ¡£ (Global QA)": return "è¯·é€‰æ‹©å…·ä½“æ–‡æ¡£...", []

    store = known_collections.get(collection_name, milvus_store)
    text = store.get_document_content(filename)
    
    if text:
        summary = ernie.generate_summary(text[:3000])
    else:
        summary = "æ— æ³•è·å–å†…å®¹ (å¯èƒ½æ˜¯çº¯å›¾ç‰‡æ–‡æ¡£æˆ–è§£æå¤±è´¥)"
    
    images = []
    file_img_path = os.path.join(ASSET_DIR, collection_name, os.path.splitext(filename)[0])
    
    if os.path.exists(file_img_path):
        for img_file in sorted(os.listdir(file_img_path)):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                
                if "formula" in img_file.lower() or "img_in_for" in img_file.lower():
                    continue
              
                full_path = os.path.join(file_img_path, img_file)
               
                images.append((full_path, img_file))
                
    return f"ğŸ“„ **{filename}**\n\n{summary}", images

def update_file_list(collection_name):
    ready, msg = check_ready()
    # å¦‚æœç³»ç»Ÿæœªå°±ç»ªï¼Œè¿”å›ç©ºåˆ—è¡¨
    if not ready: return gr.update(choices=[], label="2. æ–‡æ¡£ (æœªè¿æ¥)")
    
    store = known_collections.get(collection_name, milvus_store)
    if not store: return gr.update(choices=[], label="2. æ–‡æ¡£ (åº“ä¸å­˜åœ¨)")
    
    # è·å–æ–‡ä»¶åˆ—è¡¨
    files = store.list_documents()
    count = len(files) # ç»Ÿè®¡æ•°é‡
    
    choices = ["å…¨éƒ¨æ–‡æ¡£ (Global QA)"] + files
    # åœ¨ label ä¸­æ˜¾ç¤º (å…± N ä¸ª)
    return gr.update(choices=choices, value=choices[0], label=f"2. æ–‡æ¡£ (å…± {count} ä¸ª)")

def update_file_list_for_delete(collection_name):
    ready, msg = check_ready()
    if not ready or not collection_name: 
        return gr.update(choices=[], label="é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶")
        
    store = known_collections.get(collection_name, milvus_store)
    # è·å–æ–‡ä»¶åˆ—è¡¨
    files = store.list_documents()
    count = len(files) # ç»Ÿè®¡æ•°é‡
    return gr.update(choices=files, value=None, label=f"é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶ (å½“å‰åº“å…± {count} ä¸ª)")
def run_recall_test(collection_name):
    ready, msg = check_ready()
    if not ready: return msg
    if not collection_name: return "âŒ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªçŸ¥è¯†åº“"

    store = known_collections.get(collection_name, milvus_store)
    
    return store.test_self_recall(sample_size=20)
def create_collection_ui(new_name):
    ready, msg = check_ready()
    if not ready: return gr.update(), msg
    if not new_name: return gr.update(), "âŒ åç§°ä¸èƒ½ä¸ºç©º"
    try:
        new_store = MilvusVectorStore(
            uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"),
            collection_name=new_name, embedding_service_url="https://aistudio.baidu.com/llm/lmapi/v3",
            qianfan_api_key=os.environ.get("AISTUDIO_ACCESS_TOKEN")
        )
        dummy = [{"filename":"_init","page":0,"content":"init","chunk_id":0}]
        new_store.insert_documents(dummy)
        known_collections[new_name] = new_store
        updated = list(known_collections.keys())
        return gr.update(choices=updated, value=new_name), f"âœ… åˆ›å»ºæˆåŠŸ: {new_name}"
    except Exception as e:
        return gr.update(), f"âŒ åˆ›å»ºå¤±è´¥: {e}"

def delete_collection_ui(name):
    ready, msg = check_ready()
    if not ready: return gr.update(), msg
    if not name: return gr.update(), "è¯·é€‰æ‹©"
    try:
        alias = "delete_conn"
        connections.connect(alias=alias, uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"))
        if utility.has_collection(name, using=alias): utility.drop_collection(name, using=alias)
        connections.disconnect(alias)
        if name in known_collections: del known_collections[name]
        img_path = os.path.join(ASSET_DIR, name)
        if os.path.exists(img_path): shutil.rmtree(img_path)
        updated = list(known_collections.keys())
        val = updated[0] if updated else None
        return gr.update(choices=updated, value=val), f"ğŸ—‘ï¸ å·²åˆ é™¤: {name}"
    except Exception as e:
        return gr.update(), f"âŒ åˆ é™¤å¤±è´¥: {e}"

def delete_single_file(collection_name, filename):
    ready, msg = check_ready()
    if not ready: return msg
    if not collection_name: return "âŒ è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“"
    if not filename: return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶"
    
    store = known_collections.get(collection_name, milvus_store)
    msg = store.delete_document(filename)
    
    try:
        img_dir = os.path.join(ASSET_DIR, collection_name, os.path.splitext(filename)[0])
        if os.path.exists(img_dir):
            import shutil
            shutil.rmtree(img_dir)
            msg += " (å…³è”å›¾ç‰‡å·²æ¸…ç†)"
    except: pass
    
    return msg

def refresh_all_dropdowns():
    if not system_ready: return gr.update(), gr.update(), gr.update()#, gr.update()
    new_cols = scan_remote_collections()
    # return (gr.update(choices=new_cols), gr.update(choices=new_cols), gr.update(choices=new_cols), gr.update(choices=new_cols))
    return (
        gr.update(choices=new_cols), 
        gr.update(choices=new_cols), 
        gr.update(choices=new_cols)
    )

custom_css = """
/* 1. è¦†ç›– Gradio å…¨å±€é¢œè‰²å˜é‡ (ä¿æŒå…¨ç™½é£æ ¼) */
:root, body, .gradio-container {
    --body-background-fill: #ffffff !important;
    --background-fill-primary: #ffffff !important;
    --background-fill-secondary: #ffffff !important;
    --block-background-fill: #ffffff !important;
    --panel-background-fill: #ffffff !important;
    background-color: #ffffff !important;
}

/* 2. å¼ºåˆ¶å®¹å™¨èƒŒæ™¯ä¸ºç™½è‰² */
.gr-group, .gr-box, .gr-panel, .gr-row, .gr-column, .gr-block {
    background-color: #ffffff !important;
    border: 1px solid #e5e7eb;
    border-radius: 12px;
}

/* 3. ğŸŸ¢ [æ ¸å¿ƒä¿®å¤] æ’é™¤ Checkbox å’Œ Radioï¼Œé˜²æ­¢å‹¾é€‰çŠ¶æ€çœ‹ä¸è§ */
textarea, select, .gr-input, .gr-form, .wrap, input:not([type="checkbox"]):not([type="radio"]) {
    background-color: #ffffff !important;
}

/* 4. ä¿®å¤ä¸Šä¼ åŒºåŸŸ */
.upload-container, .drop-zone {
    background-color: #ffffff !important;
}

/* 5. é¡¶éƒ¨ Banner */
.header-banner {
    background: linear-gradient(135deg, #2563eb, #3b82f6);
    color: white;
    padding: 1.5rem;
    border-radius: 12px;
    margin-bottom: 15px;
}
.header-title { font-size: 1.5rem; font-weight: 700; }
.chatbot-container { min-height: 600px !important; }
"""
theme = gr.themes.Soft(primary_hue="blue", secondary_hue="slate")

with gr.Blocks(title="å¤šæ–‡æ¡£æ™ºèƒ½åˆ†æä¸é—®ç­”ç³»ç»Ÿ", theme=theme, css=custom_css) as demo:
    gr.HTML("""
        <div class="header-banner">
            <div class="header-title">ğŸš€ å¤šæ–‡æ¡£æ™ºèƒ½åˆ†æä¸é—®ç­”ç³»ç»Ÿ (PaddleOCRé«˜ç²¾åº¦ç‰ˆ)</div>
            <div class="header-subtitle"> PaddleOCR  Â· ERNIE 4.5 Â· Milvus</div>
        </div>
    """)
    image_context_state = gr.State("")

    with gr.Tabs():
        with gr.TabItem("ğŸ’¡ æ™ºèƒ½é—®ç­”"):
            with gr.Group():
                with gr.Row():
                    qa_col_select = gr.Dropdown(label="1. çŸ¥è¯†åº“", choices=[], scale=3)
                    qa_file_select = gr.Dropdown(label="2. æ–‡æ¡£", choices=["å…¨éƒ¨æ–‡æ¡£ (Global QA)"], value="å…¨éƒ¨æ–‡æ¡£ (Global QA)", scale=4)
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°", scale=1)
                
                with gr.Row():
                    with gr.Column(scale=6):
                        chatbot = gr.Chatbot(label="å¯¹è¯", height=650, show_label=False, elem_classes="chatbot-container", type='messages')
                        with gr.Row():
                            # upload_img_btn = gr.UploadButton("ğŸ“·", file_types=["image"], scale=1, size="sm")
                            msg = gr.Textbox(show_label=False, placeholder="è¾“å…¥é—®é¢˜...", scale=10, autofocus=True)
                            submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
                        with gr.Row():
                            qa_metric = gr.Label(label="ç½®ä¿¡åº¦", value="N/A", scale=1)
                            clear_btn = gr.ClearButton([msg, chatbot, image_context_state], value="ğŸ§¹", size="sm", scale=1)

                    with gr.Column(scale=4):
                        doc_summary = gr.Markdown(value="ğŸ‘ˆ è¯·é€‰æ‹©æ–‡æ¡£...", elem_classes="gr-box")
                        doc_gallery = gr.Gallery(label="PaddleX æå–å›¾è¡¨", show_label=False, height=400, object_fit="contain")

            refresh_btn.click(update_file_list, inputs=[qa_col_select], outputs=[qa_file_select])
            qa_col_select.change(update_file_list, inputs=[qa_col_select], outputs=[qa_file_select])
            qa_file_select.change(analyze_doc_and_images, inputs=[qa_col_select, qa_file_select], outputs=[doc_summary, doc_gallery])
            
            # upload_img_btn.upload(handle_image_upload, inputs=[upload_img_btn, chatbot], outputs=[chatbot, image_context_state])
            msg.submit(chat_respond, inputs=[msg, chatbot, qa_col_select, qa_file_select, image_context_state], outputs=[chatbot, chatbot, msg, qa_metric, image_context_state])
            submit_btn.click(chat_respond, inputs=[msg, chatbot, qa_col_select, qa_file_select, image_context_state], outputs=[chatbot, chatbot, msg, qa_metric, image_context_state])

        with gr.TabItem("ğŸ› ï¸ çŸ¥è¯†åº“ç®¡ç†"):
            with gr.Row():
                with gr.Column(scale=1):
                    # æ¨¡å—: ä¸Šä¼ æ–‡æ¡£
                    with gr.Group():
                        gr.Markdown("### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
                        # è¿™ä¸ªä¸‹æ‹‰æ¡†åŒæ—¶æ§åˆ¶ä¸Šä¼ å’Œæµ‹è¯•
                        upload_col_select = gr.Dropdown(label="ç›®æ ‡ Collection", allow_custom_value=True, choices=[])
                        # OCR è¯­è¨€é€‰æ‹©
                        ocr_lang_select = gr.Radio(
                            choices=["ä¸­è‹±æ–‡é€šç”¨ (é»˜è®¤)", "çº¯è‹±æ–‡ (English)"], 
                            value="ä¸­è‹±æ–‡é€šç”¨ (é»˜è®¤)", 
                            label="OCR æ¨¡å‹è¯­è¨€ (çº¯è‹±æ–‡æ–‡æ¡£å»ºè®®åˆ‡æ¢)"
                        )
                        # ============================
                        files_input = gr.File(label="PDFæ–‡ä»¶", file_count="multiple", type="filepath")
                        upload_btn = gr.Button("PaddleX æ™ºèƒ½è§£æ (V3)", variant="primary")
                        stop_btn = gr.Button("ğŸ›‘ ç»ˆæ­¢", variant="stop", scale=1)
                        upload_log = gr.Textbox(label="æ—¥å¿—", lines=4)
                    
                    # æ¨¡å—: ç´¢å¼•æµ‹è¯•
                    with gr.Group():
                        gr.Markdown("### ğŸ§ª ç´¢å¼•è´¨é‡è‡ªæµ‹ (FLAT)")
                        with gr.Row():
                            test_recall_btn = gr.Button("ğŸš€ è¿è¡Œå¬å›æµ‹è¯•", variant="secondary", scale=3)
                        # ç»“æœæ˜¾ç¤ºæ¡†
                        test_result_box = gr.Textbox(show_label=False, lines=2)

                with gr.Column(scale=1):
                    with gr.Group():
                        gr.Markdown("### âš™ï¸ åº“ç®¡ç†æ“ä½œ")
                        
                        gr.Markdown("#### ğŸ†• æ–°å»ºçŸ¥è¯†åº“")
                        with gr.Row():
                            new_col_name = gr.Textbox(show_label=False, scale=3)
                            create_btn = gr.Button("åˆ›å»º", variant="secondary", scale=1)
                        create_msg = gr.Label(show_label=False)
                        
                        gr.Markdown("---")
                        gr.Markdown("#### ğŸ“„ åˆ é™¤æŒ‡å®šæ–‡æ¡£")
                        with gr.Row():
                            # ä¸‹æ‹‰æ¡†ï¼šé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶
                            del_file_select = gr.Dropdown(show_label=False, choices=[], scale=3, allow_custom_value=True)
                            btn_del_file = gr.Button("åˆ é™¤æ–‡ä»¶", variant="stop", scale=1)
                        del_file_msg = gr.Textbox(show_label=False, lines=1, interactive=False)

                        gr.Markdown("---")
                        gr.Markdown("#### ğŸ—‘ï¸ åˆ é™¤çŸ¥è¯†åº“")
                        with gr.Row():
                            del_col_select = gr.Dropdown(show_label=False, choices=[], scale=3)
                            del_btn = gr.Button("åˆ é™¤", variant="stop", scale=1)

            # === ğŸ”½ äº‹ä»¶ç»‘å®šåŒºåŸŸ ===
            # 1. ä¸Šä¼ 
            upload_event = upload_btn.click(process_uploaded_pdf, inputs=[files_input, upload_col_select, ocr_lang_select], outputs=[upload_log])
            stop_btn.click(fn=lambda: None, inputs=None, outputs=None, cancels=[upload_event])
            # 2. åˆ·æ–°
            upload_event.then(refresh_all_dropdowns, outputs=[qa_col_select, upload_col_select, del_col_select])\
                        .then(update_file_list, inputs=[qa_col_select], outputs=[qa_file_select])
            # 3. åˆ›å»º
            create_btn.click(create_collection_ui, inputs=[new_col_name], outputs=[upload_col_select, create_msg])\
                      .then(refresh_all_dropdowns, outputs=[qa_col_select, upload_col_select, del_col_select])
            # 4. åˆ é™¤
            # del_btn.click(delete_collection_ui, inputs=[del_col_select], outputs=[upload_col_select, create_msg])\
            #        .then(refresh_all_dropdowns, outputs=[qa_col_select, upload_col_select, del_col_select])
            btn_del_file.click(
                fn=delete_single_file,                  # 1. æ‰§è¡Œåˆ é™¤é€»è¾‘
                inputs=[upload_col_select, del_file_select], # ä¼ å…¥ï¼šå½“å‰é€‰ä¸­çš„åº“åã€è¦åˆ é™¤çš„æ–‡ä»¶å
                outputs=[del_file_msg]                  # è¾“å‡ºï¼šæç¤ºä¿¡æ¯
            ).then(
                fn=update_file_list_for_delete,         # 2. åˆ é™¤æˆåŠŸåï¼Œè‡ªåŠ¨åˆ·æ–°åˆ—è¡¨
                inputs=[upload_col_select],             # ä¼ å…¥ï¼šå½“å‰åº“å
                outputs=[del_file_select]               # è¾“å‡ºï¼šæ›´æ–°ä¸‹æ‹‰æ¡†ï¼ˆæŠŠå·²åˆ é™¤çš„æ–‡ä»¶ç§»é™¤ï¼‰
            )
            upload_col_select.change(
                update_file_list_for_delete,   # è°ƒç”¨åˆšæ‰ä¿®æ”¹è¿‡çš„å‡½æ•°
                inputs=[upload_col_select],    # è¾“å…¥ï¼šå½“å‰é€‰ä¸­çš„çŸ¥è¯†åº“
                outputs=[del_file_select]      # è¾“å‡ºï¼šæ›´æ–°åˆ é™¤æ–‡ä»¶çš„ä¸‹æ‹‰æ¡†ï¼ˆå«æ•°é‡æ ‡é¢˜ï¼‰
            )
            # 5. å¬å›ç‡æµ‹è¯•
            test_recall_btn.click(
                run_recall_test, 
                inputs=[upload_col_select], 
                outputs=[test_result_box]
            )

        # with gr.TabItem("âš™ï¸ ç³»ç»Ÿé…ç½®"):
        #     with gr.Group():
        #         tk_aistudio = gr.Textbox(label="AISTUDIO_ACCESS_TOKEN", type="password", value=os.getenv("AISTUDIO_ACCESS_TOKEN", ""))
        #         tk_qianfan = gr.Textbox(label="QIANFAN_API_KEY", type="password", value=os.getenv("QIANFAN_API_KEY", ""))
        #         tk_uri = gr.Textbox(label="MILVUS_URI", value=os.getenv("MILVUS_URI", ""))
        #         tk_token = gr.Textbox(label="MILVUS_TOKEN", type="password", value=os.getenv("MILVUS_TOKEN", ""))
        #         btn_connect = gr.Button("è¿æ¥", variant="primary")
        #         connect_log = gr.Textbox(label="çŠ¶æ€", interactive=False)
        #         btn_connect.click(initialize_system, inputs=[tk_aistudio, tk_qianfan, tk_uri, tk_token], outputs=[connect_log, qa_col_select, upload_col_select, del_col_select])
        with gr.TabItem("âš™ï¸ ç³»ç»Ÿé…ç½®"):
            with gr.Group():
                gr.Markdown("### ğŸ”Œ è¿æ¥è®¾ç½®")
                
                # === 1. å¢åŠ æœ¬åœ°æ¨¡å¼å¼€å…³ ===
                use_local_mode = gr.Checkbox(
                    label="ğŸ“‚ å¯ç”¨æœ¬åœ°ç¦»çº¿æ¨¡å¼ (Milvus Lite)", 
                    value=False,
                    info="å‹¾é€‰åï¼Œæ•°æ®å°†ä¿å­˜åœ¨æœ¬åœ° .db æ–‡ä»¶ä¸­ï¼Œæ— éœ€ Milvus æœåŠ¡å™¨ã€‚"
                )
                
                # === 2. è¾“å…¥æ¡†åŒºåŸŸ ===
                with gr.Row():
                    # API Key å§‹ç»ˆéœ€è¦ (ç”¨äº PaddleX å’Œ Embedding)
                    tk_aistudio = gr.Textbox(label="AISTUDIO_ACCESS_TOKEN", type="password", value=os.getenv("AISTUDIO_ACCESS_TOKEN", ""), scale=1)
                    tk_qianfan = gr.Textbox(label="QIANFAN_API_KEY", type="password", value=os.getenv("QIANFAN_API_KEY", ""), scale=1)
                
                with gr.Row():
                    # URI å’Œ Token ä¼šæ ¹æ®ä¸Šé¢çš„å¼€å…³å˜åŒ–
                    tk_uri = gr.Textbox(label="MILVUS_URI", value=os.getenv("MILVUS_URI", ""), placeholder="ä¾‹å¦‚: http://localhost:19530", scale=1)
                    tk_token = gr.Textbox(label="MILVUS_TOKEN", type="password", value=os.getenv("MILVUS_TOKEN", ""), scale=1)
                
                btn_connect = gr.Button("è¿æ¥ / åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary")
                connect_log = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", interactive=False, lines=2)

                # === 3. äº¤äº’é€»è¾‘ï¼šåˆ‡æ¢æ¨¡å¼ ===
                def toggle_mode(is_local):
                    if is_local:
                        # åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼ï¼šå¡«å…¥æœ¬åœ°æ–‡ä»¶åï¼Œæ¸…ç©ºå¹¶ç¦ç”¨ Token
                        return (
                            gr.update(value="./my_knowledge_base.db", interactive=False, label="æœ¬åœ°æ•°æ®åº“è·¯å¾„"), 
                            gr.update(value="", interactive=False, placeholder="æœ¬åœ°æ¨¡å¼æ— éœ€ Token")
                        )
                    else:
                        # åˆ‡æ¢å›æœåŠ¡å™¨æ¨¡å¼ï¼šæ¢å¤é»˜è®¤å€¼ï¼Œå¯ç”¨è¾“å…¥
                        return (
                            gr.update(value=os.getenv("MILVUS_URI", ""), interactive=True, label="MILVUS_URI"), 
                            gr.update(value=os.getenv("MILVUS_TOKEN", ""), interactive=True, placeholder="")
                        )

                use_local_mode.change(toggle_mode, inputs=[use_local_mode], outputs=[tk_uri, tk_token])

                # === 4. è¿æ¥æŒ‰é’® (ä¿æŒåŸé€»è¾‘ï¼Œåªéœ€å‡å°‘ outputs æ•°é‡ä»¥ä¿®å¤ä¹‹å‰çš„è­¦å‘Š) ===
                btn_connect.click(
                    initialize_system, 
                    inputs=[tk_aistudio, tk_qianfan, tk_uri, tk_token], 
                    # æ³¨æ„ï¼šè¿™é‡Œåªä¿ç•™äº† 4 ä¸ª outputï¼Œä¿®å¤äº†ä¹‹å‰çš„è­¦å‘Š
                    outputs=[connect_log, qa_col_select, upload_col_select, del_col_select]
                )
def find_free_port(start=7860):
    for port in range(start, start+10):
        try:
            s = socket.socket()
            s.bind(('', port))
            s.close()
            return port
        except OSError: continue
    return start

if __name__ == "__main__":
    port = find_free_port()
    print(f"ğŸš€ å¯åŠ¨ UI: http://127.0.0.1:{port}")
    demo.launch(server_name="127.0.0.1", server_port=port, inbrowser=True)