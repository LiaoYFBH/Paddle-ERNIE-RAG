import os
import sys
import logging
import shutil
import base64
import time
import requests
import json
import re
import binascii
from dotenv import load_dotenv

# å¼•å…¥å·¥å…·ç±»
try:
    from utils.vector_store import MilvusVectorStore
    from utils.ernie_client import ERNIEClient
    from utils.reranker_v2 import RerankerAndFilterV2
except ImportError as e:
    print(f"âŒ å¯¼å…¥å·¥å…·ç±»å¤±è´¥: {e}")
    # ä¸ºäº†é˜²æ­¢æŠ¥é”™å¯¼è‡´ç¨‹åºå´©æºƒï¼Œè¿™é‡Œå¯ä»¥åšä¸ªè½¯å¤„ç†æˆ–ç›´æ¥é€€å‡º
    # exit(1) 

from pymilvus import utility, connections
import gradio as gr

load_dotenv()
def on_gallery_select(evt: gr.SelectData, collection_name, doc_filename):
    """
    ç”¨æˆ·ç‚¹å‡»å›¾ç‰‡æ—¶ï¼Œè§£æå‡ºï¼šè·¯å¾„ã€æ–‡æ¡£åã€é¡µç 
    """
    if not collection_name or not doc_filename:
        return None, "âš ï¸ è¯·å…ˆé€‰æ‹©æ–‡æ¡£"
    
    try:
        # 1. å®šä½å›¾ç‰‡ç›®å½•
        file_img_path = os.path.join(ASSET_DIR, collection_name, os.path.splitext(doc_filename)[0])
        valid_images = []
        if os.path.exists(file_img_path):
            # å¿…é¡»ä¸ analyze_doc_and_images æ’åºä¸€è‡´
            for img_file in sorted(os.listdir(file_img_path)):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    if "formula" in img_file.lower() or "img_in_for" in img_file.lower():
                        continue
                    full_path = os.path.join(file_img_path, img_file)
                    valid_images.append(full_path)
        
        # 2. è·å–é€‰ä¸­çš„å›¾ç‰‡è·¯å¾„
        if 0 <= evt.index < len(valid_images):
            selected_img_path = valid_images[evt.index]
            filename_only = os.path.basename(selected_img_path)
            
            # 3. ğŸŸ¢ æ ¸å¿ƒï¼šä»æ–‡ä»¶åè§£æé¡µç  (æ ¼å¼: p0_123456_name.jpg)
            # p(\d+) åŒ¹é… p0, p1, p10...
            page_match = re.match(r"p(\d+)_", filename_only)
            page_num = int(page_match.group(1)) + 1 if page_match else "æœªçŸ¥"
            
            print(f"ğŸ–¼ï¸ é€‰ä¸­å›¾ç‰‡: {filename_only} (ç¬¬ {page_num} é¡µ)")
            
            # 4. æ„é€ å®Œæ•´ä¸Šä¸‹æ–‡åŒ… (Dict) è€Œä¸æ˜¯ç®€å•çš„ String
            img_context_data = {
                "path": selected_img_path,
                "doc_name": doc_filename,
                "page": page_num,
                "collection": collection_name
            }
            
            return img_context_data, f"âœ… å·²é€‰ä¸­ç¬¬ {page_num} é¡µçš„å›¾è¡¨ï¼Œå¯è¯¢é—®è¯¦æƒ…ï¼"
        
        return None, "âŒ æ— æ³•å®šä½å›¾ç‰‡"
    except Exception as e:
        print(f"âŒ å›¾ç‰‡é€‰æ‹©å¼‚å¸¸: {e}")
        return None, f"é€‰æ‹©å‡ºé”™: {e}"
def encode_name(ui_name):
    """æŠŠä¸­æ–‡åç§°è½¬ä¸º Milvus åˆæ³•çš„ Hex å­—ç¬¦ä¸² (ä¾‹å¦‚: 'æµ‹è¯•' -> 'kb_e6b58b...')"""
    if not ui_name: return ""
    # å¦‚æœæœ¬èº«å°±æ˜¯çº¯è‹±æ–‡/æ•°å­—/ä¸‹åˆ’çº¿ï¼Œä¸”ç¬¦åˆè§„èŒƒï¼Œç›´æ¥è¿”å› (å…¼å®¹æ—§åº“)
    if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', ui_name):
        return ui_name
    
    # å¦åˆ™è¿›è¡Œ Hex ç¼–ç ï¼Œå¹¶åŠ å‰ç¼€ kb_ ä¿è¯å­—æ¯å¼€å¤´
    hex_str = binascii.hexlify(ui_name.encode('utf-8')).decode('utf-8')
    return f"kb_{hex_str}"

def decode_name(real_name):
    """æŠŠ Hex å­—ç¬¦ä¸²è½¬å›ä¸­æ–‡"""
    if not real_name: return ""
    if real_name.startswith("kb_"):
        try:
            # å»æ‰å‰ç¼€ï¼Œå°è¯•åè§£
            hex_str = real_name[3:]
            return binascii.unhexlify(hex_str).decode('utf-8')
        except:
            # è§£ç å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç”¨æˆ·è‡ªå·±æ‰‹åŠ¨å»ºçš„ä»¥ kb_ å¼€å¤´çš„è‹±æ–‡åº“ï¼‰ï¼Œè¿”å›åŸå
            return real_name
    return real_name
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å±è”½æ— å…³æ—¥å¿—
silence_libs = ["httpx", "httpcore", "urllib3"]
for lib in silence_libs:
    logging.getLogger(lib).setLevel(logging.ERROR)

# ç›®å½•å‡†å¤‡
ASSET_DIR = "assets"
os.makedirs(ASSET_DIR, exist_ok=True)

# === å…¨å±€çŠ¶æ€å˜é‡ ===
ernie = None
milvus_store = None
reranker_filter = None
known_collections = {}
system_ready = False

# === æ ¸å¿ƒç±»: åœ¨çº¿ PDF è§£æå™¨ ===
class OnlinePDFParser:
    """å¤„ç†äº‘ç«¯ API è°ƒç”¨"""
    def __init__(self, api_url, token):
        self.api_url = api_url
        self.token = token

    def predict(self, file_path):
        if not self.token:
            return None, "âŒ é”™è¯¯: æœªé…ç½® Token"
        if not self.api_url:
            return None, "âŒ é”™è¯¯: æœªé…ç½® API URL"
        
        file_name = os.path.basename(file_path)
        print(f"â˜ï¸ [Online] æ­£åœ¨è¯·æ±‚åœ¨çº¿ OCR API: {file_name}")
        
        try:
            with open(file_path, "rb") as file:
                file_bytes = file.read()
                file_data = base64.b64encode(file_bytes).decode("ascii")

            # ç®€å•åˆ¤æ–­æ–‡ä»¶ç±»å‹
            ext = os.path.splitext(file_name)[1].lower()
            file_type = 0 if ext == '.pdf' else 1 
            
            payload = {
                "file": file_data,
                "fileType": file_type,
                "useDocOrientationClassify": False,
                "useDocUnwarping": False,
                "useTextlineOrientation": False,
                "useChartRecognition": False,
            }
            
            headers = {
                "Authorization": f"token {self.token}", 
                "Content-Type": "application/json"
            }
            
            # å¤§æ–‡ä»¶ä¸Šä¼ éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¶…æ—¶è®¾ä¸º 600ç§’
            response = requests.post(self.api_url, json=payload, headers=headers, timeout=600)
            
            if response.status_code != 200:
                print(f"âŒ [API Error] HTTP {response.status_code}: {response.text[:100]}")
                return None, f"API HTTPé”™è¯¯ ({response.status_code})"
            
            res_json = response.json()
            
            if "errorCode" in res_json and res_json["errorCode"]:
                err_msg = res_json.get('errorMsg', 'æœªçŸ¥é”™è¯¯')
                print(f"âŒ [API Error] ä¸šåŠ¡é”™è¯¯: {err_msg}")
                return None, f"API ä¸šåŠ¡é”™è¯¯: {err_msg}"

            api_result = res_json.get("result", {})
            parsing_results = api_result.get("layoutParsingResults", [])
            
            if not parsing_results:
                if isinstance(api_result, list):
                     return None, "âš ï¸ æ£€æµ‹åˆ°çº¯ OCR æ¥å£è¿”å›ï¼Œæœ¬ç³»ç»Ÿéœ€è¦ Layout Parsing ç»“æ„ã€‚"
                print(f"âš ï¸ [API Warning] layoutParsingResults ä¸ºç©ºã€‚Keys: {list(res_json.keys())}")
                return None, "API è¿”å›ç»“æœä¸ºç©º (å¯èƒ½æ–‡ä»¶æ— æ³•è§£æ)"

            class MockResult:
                def __init__(self, md_text, imgs):
                    self.markdown = {
                        'markdown_texts': md_text,
                        'markdown_images': imgs
                    }

            mock_outputs = []
            
            for i, item in enumerate(parsing_results):
                md_data = item.get("markdown", {})
                raw_text = md_data.get("text", "")
                
                # å¤„ç†å›¾ç‰‡ä¸‹è½½
                image_urls = md_data.get("images", {})
                processed_images = {}
                
                if image_urls:
                    print(f"   â†³ æ­£åœ¨ä¸‹è½½ç¬¬ {i+1} éƒ¨åˆ†çš„ {len(image_urls)} å¼ å›¾ç‰‡...")
                    for img_key, img_url in image_urls.items():
                        try:
                            img_resp = requests.get(img_url, timeout=30)
                            if img_resp.status_code == 200:
                                b64_str = base64.b64encode(img_resp.content).decode('utf-8')
                                processed_images[img_key] = b64_str
                        except Exception as e: pass
                
                mock_outputs.append(MockResult(raw_text, processed_images))
            
            return mock_outputs, "Success"

        except Exception as e:
            return None, f"è¯·æ±‚å¼‚å¸¸: {str(e)}"

# === æ–‡æœ¬å¤„ç†å·¥å…· ===
def split_text_into_chunks(text: str, chunk_size: int = 300, overlap: int = 120) -> list:
    if not text: return []
    lines = [line.strip() for line in text.split("\n") if line.strip()]
    chunks = []
    current_chunk = []
    current_length = 0
    for line in lines:
        while len(line) > chunk_size:
            part = line[:chunk_size]
            line = line[chunk_size:]
            if current_length + len(part) > chunk_size and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(part)
            current_length += len(part)
        if current_length + len(line) > chunk_size and current_chunk:
            chunks.append("\n".join(current_chunk))
            overlap_text = current_chunk[-1][-overlap:] if current_chunk else ""
            current_chunk = [overlap_text] if overlap_text else []
            current_length = len(overlap_text)
        current_chunk.append(line)
        current_length += len(line)
    if current_chunk:
        content = "\n".join(current_chunk).strip()
        if content: chunks.append(content)
    return chunks

def check_ready():
    if not system_ready: return False, "âš ï¸ ç³»ç»Ÿæœªè¿æ¥"
    return True, ""

def scan_remote_collections():
    global known_collections, ernie
    try:
        alias = f"scan_{int(time.time())}"
        connections.connect(alias=alias, uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"))
        all_colls = utility.list_collections(using=alias)
        connections.disconnect(alias)
        for real_name in all_colls:
            # ğŸŸ¢ è§£ç ï¼šè·å– UI æ˜¾ç¤ºç”¨çš„åå­—
            ui_name = decode_name(real_name)
            
            if ui_name not in known_collections:
                # ğŸŸ¢ å…³é”®ï¼šå­—å…¸ Key ç”¨ä¸­æ–‡(ui_name)ï¼Œä½†ä¼ ç»™ Milvus çš„å‚æ•°ç”¨çœŸå(real_name)
                known_collections[ui_name] = MilvusVectorStore(
                    uri=os.environ.get("MILVUS_URI"), 
                    token=os.environ.get("MILVUS_TOKEN"),
                    collection_name=real_name,  # <--- è¿™é‡Œå¿…é¡»æ˜¯ encoded çš„çœŸå
                    embedding_client=ernie
                )
        # for name in all_colls:
            # if name not in known_collections:
                # ä¼ å…¥å…¨å±€é…ç½®çš„ ernie å®¢æˆ·ç«¯
                # known_collections[name] = MilvusVectorStore(
                #     uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"),
                #     collection_name=name, embedding_client=ernie
                # )
        return list(known_collections.keys())
    except:
        return list(known_collections.keys())

def initialize_system(
    llm_api_base, llm_api_key, llm_model,
    embed_api_base, embed_api_key, embed_model,
    ocr_url, ocr_token,
    milvus_uri, milvus_token,
    api_qps 
):
    global ernie, milvus_store, reranker_filter, system_ready, known_collections

    # 1. åŸºç¡€æ¸…ç†
    milvus_uri = milvus_uri.strip() if milvus_uri else ""
    milvus_token = milvus_token.strip() if milvus_token else ""

    is_local_mode = milvus_uri.endswith(".db")
    # æ£€æŸ¥åŸºæœ¬å¿…è¦å‚æ•° (LLM Key, Embed Key, Milvus URI)
    basic_check = all([llm_api_key, embed_api_key, milvus_uri])
    token_check = True if is_local_mode else bool(milvus_token)

    if not (basic_check and token_check):
        return "âŒ è¯·å¡«å†™å¿…è¦ä¿¡æ¯ (LLM Key, Embed Key, Milvus URI)", gr.update(), gr.update(), gr.update()

    try:
        # 2. è®¾ç½®ç¯å¢ƒå˜é‡ (ä¾›å…¶ä»–æ¨¡å—æˆ–æŒä¹…åŒ–ä½¿ç”¨)
        os.environ["MILVUS_URI"] = milvus_uri
        if milvus_token: os.environ["MILVUS_TOKEN"] = milvus_token
        
        # å­˜å‚¨ OCR é…ç½®åˆ° Envï¼Œä»¥ä¾¿ä¸Šä¼ æ—¶ä½¿ç”¨
        if ocr_url: os.environ["OCR_API_URL"] = ocr_url
        if ocr_token: os.environ["OCR_ACCESS_TOKEN"] = ocr_token

        # 3. Milvus é‡è¿æ¸…ç†
        try:
            if connections.has_connection("default"):
                connections.disconnect("default")
        except: pass

        known_collections = {}
        
        # 4. åˆå§‹åŒ– ERNIE Client (ä¼ å…¥ QPS)
        ernie = ERNIEClient(
            llm_api_base=llm_api_base,
            llm_api_key=llm_api_key,
            llm_model=llm_model,
            embed_api_base=embed_api_base,
            embed_api_key=embed_api_key,
            embed_model=embed_model,
            qps=api_qps
        )
        
        reranker_filter = RerankerAndFilterV2()

        # 5. åˆå§‹åŒ– Milvus Store (ä¼ å…¥é…ç½®å¥½çš„ ernie client)
        # milvus_store = MilvusVectorStore(
        #     uri=milvus_uri,
        #     token=milvus_token, 
        #     collection_name="é»˜è®¤çŸ¥è¯†åº“", 
        #     embedding_client=ernie 
        # )
        
        # known_collections = {milvus_store.collection_name: milvus_store}
        default_ui_name = "é»˜è®¤çŸ¥è¯†åº“"
        default_real_name = encode_name(default_ui_name)

        milvus_store = MilvusVectorStore(
            uri=milvus_uri,
            token=milvus_token, 
            collection_name=default_real_name, # ä½¿ç”¨ç¼–ç åçš„åå­—
            embedding_client=ernie 
        )
        
        # å­˜å…¥å­—å…¸
        known_collections = {default_ui_name: milvus_store}
        try: scan_remote_collections()
        except: pass
        
        cols = list(known_collections.keys())
        default_col = cols[0] if cols else None
        
        system_ready = True
        return (
            f"âœ… è¿æ¥æˆåŠŸ (QPS: {api_qps})", 
            gr.update(choices=cols, value=default_col),
            gr.update(choices=cols, value=default_col),
            gr.update(choices=cols, value=default_col)
        )
    except Exception as e:
        return f"âŒ å¤±è´¥: {str(e)}", gr.update(), gr.update(), gr.update()

def process_uploaded_pdf(files, collection_name, progress=gr.Progress()):
    if collection_name: collection_name = str(collection_name).strip()
    
    ready, msg = check_ready()
    if not ready: return msg
    if not files: return "âš ï¸ è¯·ä¸Šä¼  PDF"
    
    if collection_name not in known_collections:
        create_collection_ui(collection_name)
    
    target_store = known_collections[collection_name]
    results = [] 
    col_img_dir = os.path.join(ASSET_DIR, collection_name)
    try: os.makedirs(col_img_dir, exist_ok=True)
    except: pass
    
    # ä»å…¨å±€é…ç½®è¯»å– OCR è®¾ç½®
    print(f"\n[System] åˆå§‹åŒ–åœ¨çº¿ API è§£æå™¨...")
    token = os.environ.get("OCR_ACCESS_TOKEN", os.environ.get("AISTUDIO_ACCESS_TOKEN"))
    api_url = os.environ.get("OCR_API_URL")
    
    if not api_url:
        return "âŒ é”™è¯¯: æœªé…ç½® OCR API URLï¼è¯·åœ¨ 'ç³»ç»Ÿé…ç½®' ä¸­å¡«å†™ã€‚"
    if not token:
        return "âŒ é”™è¯¯: æœªé…ç½® OCR Access Tokenï¼è¯·åœ¨ 'ç³»ç»Ÿé…ç½®' ä¸­å¡«å†™ã€‚"
        
    online_parser = OnlinePDFParser(api_url, token)

    print(f"ğŸ” æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨...")
    try: existing_files = set(target_store.list_documents())
    except: existing_files = set()

    total_files = len(files)

    # ğŸŸ¢ å…³é”®ä¿®æ”¹ï¼šæ‰‹åŠ¨æ§åˆ¶è¿›åº¦ï¼Œæ›¿ä»£è‡ªåŠ¨çš„ tqdm
    for i, file_path in enumerate(files):
        # è®¡ç®—å½“å‰æ–‡ä»¶çš„åŸºç¡€è¿›åº¦ (ä¾‹å¦‚ç¬¬ 1 ä¸ªæ–‡ä»¶ï¼ŒåŸºç¡€æ˜¯ 0.0ï¼Œç¬¬ 2 ä¸ªæ˜¯ 0.5)
        base_prog = i / total_files
        
        path_str = file_path.name if hasattr(file_path, 'name') else file_path
        filename = os.path.basename(path_str)
        abs_path = os.path.abspath(path_str)
    
        if filename in existing_files:
            results.append(f"â© {filename} (å·²å­˜åœ¨)")
            # å³ä½¿è·³è¿‡ï¼Œä¹Ÿè¦æ›´æ–°ä¸€ä¸‹è¿›åº¦
            progress((i + 1) / total_files, desc=f"[{i+1}/{total_files}] è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶: {filename}")
            continue
        
        file_img_dir = os.path.join(col_img_dir, os.path.splitext(filename)[0])
        if os.path.exists(file_img_dir): shutil.rmtree(file_img_dir)
        os.makedirs(file_img_dir, exist_ok=True)
        
        print(f"\nğŸš€ å¼€å§‹è§£ææ–‡ä»¶: {filename} (æ¨¡å¼: â˜ï¸ Online)")
        
        # ğŸŸ¢ é˜¶æ®µ 1ï¼šè¯·æ±‚äº‘ç«¯ API (è¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œï¼Œæ˜¾ç¤ºâ€œæ­£åœ¨è§£æâ€)
        # progress ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¿›åº¦æ¡ç™¾åˆ†æ¯”(0-1)ï¼Œdesc æ˜¯æ–‡å­—æè¿°
        progress(base_prog + 0.05, desc=f"[{i+1}/{total_files}] æ­£åœ¨è¯·æ±‚äº‘ç«¯ OCR è§£æ: {filename} (å¤§æ–‡ä»¶å¯èƒ½éœ€ 1-2 åˆ†é’Ÿ)...")
        
        output = []
        try:
            # åªè°ƒç”¨åœ¨çº¿
            output, err_msg = online_parser.predict(abs_path)
            if output is None:
                print(f"âŒ è§£æå¤±è´¥: {err_msg}") 
                results.append(f"âŒ {filename}: {err_msg}")
                continue
        except Exception as e:
            err_msg = f"âŒ {filename}: å¼‚å¸¸ ({str(e)})"
            print(err_msg)
            results.append(err_msg)
            continue

        # ğŸŸ¢ é˜¶æ®µ 2ï¼šå¤„ç†è§£æç»“æœ (æŒ‰é¡µæ›´æ–°è¿›åº¦æ¡)
        file_chunk_count = 0 
        if output:
            total_pages = len(output)
            for page_idx, res in enumerate(output):
                # åŠ¨æ€æ›´æ–°å­è¿›åº¦ï¼šOCR å ä¸€éƒ¨åˆ†æ—¶é—´ï¼ŒEmbedding å ä¸€éƒ¨åˆ†
                # è¿™é‡Œå‡è®¾ Embedding è¿‡ç¨‹å å½“å‰æ–‡ä»¶è¿›åº¦çš„ 80% (0.2 ~ 1.0)
                step_prog = (page_idx / total_pages) * 0.8
                current_total = base_prog + 0.2 + (step_prog / total_files)
                
                # å®æ—¶æ›´æ–°ï¼šæ˜¾ç¤ºæ­£åœ¨å¤„ç†ç¬¬å‡ é¡µ
                progress(current_total, desc=f"[{i+1}/{total_files}] æ­£åœ¨å…¥åº“: {filename} (ç¬¬ {page_idx+1}/{total_pages} é¡µ)...")

                md_data = res.markdown
                page_text = md_data.get('markdown_texts', '') 
                page_images = md_data.get('markdown_images', {})
             
                for img_path_key, img_val in page_images.items():
                    try:
                        base_name = os.path.basename(img_path_key)
                        sname = f"p{page_idx}_{int(time.time())}_{base_name}"
                        if not sname.endswith(('.jpg', '.png')): sname += ".jpg"
                        spath = os.path.join(file_img_dir, sname)

                        if isinstance(img_val, str):
                            with open(spath, "wb") as f: f.write(base64.b64decode(img_val))
                        elif hasattr(img_val, 'save'):
                            img_val.save(spath)
                        
                        page_text = page_text.replace(img_path_key, f"[å›¾è¡¨: {sname}]")
                    except Exception as e: pass
                
                if not page_text.strip(): continue

                page_chunks = split_text_into_chunks(page_text)
                
                docs = []
                for cid, chunk in enumerate(page_chunks):
                    header = f"æ–‡æ¡£: {filename} (P{page_idx+1})\n"
                    safe_limit = 380 - len(header)
                    
                    safe_chunk = chunk
                    if len(chunk) > safe_limit:
                        safe_chunk = chunk[:safe_limit] + "..."
                        
                    docs.append({
                        "filename": filename, 
                        "page": page_idx, 
                        "content": f"{header}{safe_chunk}", 
                        "chunk_id": file_chunk_count + cid
                    })
                
                if docs:
                    target_store.insert_documents(docs)
                    file_chunk_count += len(docs)

        if file_chunk_count > 0:
            success_msg = f"âœ… {filename} (æå– {file_chunk_count} ç‰‡æ®µ)"
            results.append(success_msg)
        else:
            fail_msg = f"âŒ {filename}: æœªæå–åˆ°æœ‰æ•ˆå†…å®¹"
            print(fail_msg)
            results.append(fail_msg)
            
        time.sleep(0.05)
            
    return "\n".join(results)

def ask_question_logic(question, collection_name, target_filename=None):
    ready, msg = check_ready()
    if not ready: return msg, "N/A"
    if not question.strip(): return "è¯·è¾“å…¥é—®é¢˜", "0.0%"
    
    # åŒå‘ç¿»è¯‘é€»è¾‘
    expanded_query = question
    try:
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in question)
        prompt = f"Translate the following Chinese query into English directly without explanation:\n{question}" if has_chinese else f"å°†ä»¥ä¸‹è‹±æ–‡é—®é¢˜ç›´æ¥ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¸è¦è§£é‡Šï¼š\n{question}"
        
        if ernie:
            translated_part = ernie.chat([{"role": "user", "content": prompt}])
            if translated_part:
                expanded_query = f"{question} {translated_part}"
                print(f"âœ… [Query] åŒè¯­å¢å¼ºå: {expanded_query}")
    except Exception as e: pass

    target_store = known_collections.get(collection_name, milvus_store)
    search_kwargs = {"top_k": 60}
    if target_filename and target_filename != "å…¨éƒ¨æ–‡æ¡£ (Global QA)":
        search_kwargs["expr"] = f"filename == '{target_filename}'"
        
    retrieved = target_store.search(expanded_query, **search_kwargs)
    if not retrieved: return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚", "0.0%"
    
    processed, _ = reranker_filter.process(expanded_query, retrieved)
    final = processed[:22]
    top_score = final[0].get('composite_score', 0) if final else 0
    metric = f"{min(100, top_score):.1f}%"
    
    answer = ernie.answer_question(question, final)
    seen = set()
    sources = "\n\nğŸ“š **å‚è€ƒæ¥æº:**\n"
    for c in final:
        page_num = c.get('page', 0) + 1
        fname = c.get('filename', 'æœªçŸ¥æ–‡æ¡£')
        key = f"{fname} (P{page_num})"
        if key not in seen:
            sources += f"- {key} [ç›¸å…³æ€§:{c.get('composite_score',0):.0f}%]\n"
            seen.add(key)
            
    return answer + sources, metric
def chat_respond(message, history, collection_name, target_filename, img_context_data):
    if not message: return history, history, "", "N/A", img_context_data
    
    # === å˜é‡åˆå§‹åŒ– ===
    # æˆ‘ä»¬å…ˆå‡†å¤‡å¥½é»˜è®¤çš„â€œç”¨æˆ·æé—®â€å’Œâ€œæœºå™¨å›ç­”â€å˜é‡
    # æ— è®ºèµ°å“ªæ¡è·¯ï¼Œæœ€åéƒ½åªæŠŠè¿™ä¸¤ä¸ªå˜é‡åŠ è¿› history
    user_display_text = message
    bot_response_text = ""
    metric_info = "N/A"
    
    # çŠ¶æ€æ ‡è®°
    vision_success = False
    
    # ============================================================
    # 1ï¸âƒ£ å°è¯•å¤šæ¨¡æ€ (Vision) é€šé“
    # ============================================================
    if img_context_data and isinstance(img_context_data, dict) and os.path.exists(img_context_data.get("path", "")):
        img_path = img_context_data["path"]
        page_num = img_context_data["page"]
        doc_name = img_context_data["doc_name"]
        col_name = img_context_data.get("collection")
        
        # 1.1 å‡†å¤‡èƒŒæ™¯æ–‡æœ¬
        page_text_context = ""
        try:
            store = known_collections.get(col_name, milvus_store)
            db_page_idx = int(page_num) - 1 if isinstance(page_num, int) else 0
            res = store.collection.query(
                expr=f'filename == "{doc_name}" and page == {db_page_idx}',
                output_fields=["content"], limit=3
            )
            texts = [r['content'] for r in res]
            if texts: page_text_context = "\n".join(texts)[:800]
        except: pass

        # 1.2 æ„é€  Prompt (å¦‚æœé™çº§ï¼Œè¿™ä¹Ÿå°†ä½œä¸º RAG çš„è¾“å…¥)
        final_prompt = f"""
        ã€ä»»åŠ¡ã€‘ç»“åˆå›¾ç‰‡å’ŒèƒŒæ™¯ä¿¡æ¯å›ç­”é—®é¢˜ã€‚
        ã€å›¾ç‰‡å…ƒæ•°æ®ã€‘æ¥æºï¼š{doc_name} (P{page_num})
        ã€èƒŒæ™¯æ–‡æœ¬ã€‘{page_text_context}
        ã€ç”¨æˆ·é—®é¢˜ã€‘{message}
        """

        # 1.3 è¯·æ±‚æ¨¡å‹
        try:
            print(f"ğŸ“· æ­£åœ¨è¯·æ±‚å¤šæ¨¡æ€æ¨¡å‹...")
            answer = ernie.chat_with_image(final_prompt, img_path)
            
            # åªæœ‰å½“å›ç­”æœ‰æ•ˆï¼Œä¸”ä¸åŒ…å«é”™è¯¯æç¤ºæ—¶ï¼Œæ‰ç®—æˆåŠŸ
            if answer and "å¤±è´¥" not in answer:
                # âœ… æˆåŠŸï¼æ›´æ–°æš‚å­˜å˜é‡
                user_display_text = f"[é’ˆå¯¹ P{page_num} å›¾è¡¨] {message}"
                # 1. æ„é€ æ¥æºä¿¡æ¯ (ä¿æŒä¸æ–‡æœ¬ RAG æ ¼å¼ä¸€è‡´)
                vision_source = f"\n\nğŸ“š **å‚è€ƒæ¥æº:**\n- ğŸ–¼ï¸ {doc_name} (P{page_num}) [è§†è§‰é”å®š]"
                
                # 2. æ‹¼æ¥å›ç­”
                bot_response_text = answer + vision_source
                metric_info = "Vision Mode"
                vision_success = True 
                
        except Exception as e:
            err_str = str(e).lower()
            if "400" in err_str or "invalid" in err_str or "support" in err_str:
                print(f"âš ï¸ æ¨¡å‹ä¸æ”¯æŒå›¾ç‰‡ï¼Œå‡†å¤‡åˆ‡æ¢è‡³æ–‡æœ¬æ¨¡å¼ã€‚")
            else:
                print(f"âŒ å¤šæ¨¡æ€è°ƒç”¨å¼‚å¸¸: {e}")

    # ============================================================
    # 2ï¸âƒ£ é™çº§/å¸¸è§„ RAG é€šé“ (ä»…å½“å¤šæ¨¡æ€æœªæˆåŠŸæ—¶æ‰§è¡Œ)
    # ============================================================
    if not vision_success:
        print("ğŸ”„ æ‰§è¡Œæ–‡æœ¬ RAG é€šé“")
        
        if not collection_name: 
            bot_response_text = "âš ï¸ è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“"
        else:
            # æ„é€ æŸ¥è¯¢
            full_query = message
            prefix_hint = ""
            
            # å¦‚æœä¹‹å‰å‡†å¤‡è¿‡ final_promptï¼ˆè¯´æ˜æ˜¯é™çº§ä¸‹æ¥çš„ï¼‰ï¼Œæˆ‘ä»¬å¤ç”¨å®ƒä½œä¸ºæ–‡æœ¬è¾“å…¥
            if 'final_prompt' in locals() and final_prompt:
                full_query = final_prompt
                prefix_hint = "â„¹ï¸ **ç³»ç»Ÿæç¤º**ï¼šå½“å‰æ¨¡å‹ä¸æ”¯æŒè§†è§‰è¾“å…¥ï¼Œå·²è‡ªåŠ¨æ ¹æ®å›¾è¡¨å‘¨å›´çš„æ–‡æœ¬ä¸ºæ‚¨åˆ†æã€‚\n\n"

            # æ‰§è¡Œæ£€ç´¢é—®ç­”
            answer, metric = ask_question_logic(full_query, collection_name, target_filename)
            
            # æ›´æ–°æš‚å­˜å˜é‡
            bot_response_text = prefix_hint + answer
            metric_info = metric

    # ============================================================
    # 3ï¸âƒ£ ç»Ÿä¸€å‡ºå£ (Single Exit) - ç»å¯¹é˜²æ­¢åŒé‡æ°”æ³¡
    # ============================================================
    history.append({"role": "user", "content": user_display_text})
    history.append({"role": "assistant", "content": bot_response_text})
    
    # å¦‚æœå¤šæ¨¡æ€æˆåŠŸï¼Œæˆ‘ä»¬è¦æ¸…ç©º img_context_data (è¿”å› None)
    # å¦‚æœå¤±è´¥/é™çº§ï¼Œæˆ‘ä»¬ä¹Ÿæ¸…ç©ºå®ƒï¼ˆå‡è®¾ç”¨æˆ·çš„ä¸€é—®ä¸€ç­”æ¶ˆè€—äº†è¿™æ¬¡å›¾ç‰‡é€‰æ‹©ï¼‰ï¼Œæˆ–è€…ä½ å¯ä»¥ä¿ç•™
    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©æ¶ˆè€—æ‰ï¼Œé¿å…çŠ¶æ€æ··ä¹±
    return history, "", metric_info, None
# def chat_respond(message, history, collection_name, target_filename, img_context):
#     if not message: return history, history, "", "N/A", img_context
#     if not collection_name: 
#         history.append({"role": "user", "content": message})
#         history.append({"role": "assistant", "content": "âš ï¸ è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“"})
#         return history, history, "", "N/A", img_context

#     full_query = message
#     if img_context: 
#         full_query = f"{img_context}\nç”¨æˆ·é—®é¢˜: {message}"
#         img_context = "" 
    
#     answer, metric = ask_question_logic(full_query, collection_name, target_filename)
#     history.append({"role": "user", "content": message})
#     history.append({"role": "assistant", "content": answer})
#     return history, history, "", metric, img_context

def analyze_doc_and_images(collection_name, filename):
    ready, msg = check_ready()
    if not ready: return "ç³»ç»Ÿæœªè¿æ¥", []
    if not filename or filename == "å…¨éƒ¨æ–‡æ¡£ (Global QA)": return "è¯·é€‰æ‹©å…·ä½“æ–‡æ¡£...", []

    store = known_collections.get(collection_name, milvus_store)
    text = store.get_document_content(filename)
    
    if text:
        summary = ernie.generate_summary(text[:3000])
    else:
        summary = "æ— æ³•è·å–å†…å®¹ (å¯èƒ½æ˜¯çº¯å›¾ç‰‡æ–‡æ¡£æˆ–è§£æå¤±è´¥)"
    
    images = []
    file_img_path = os.path.join(ASSET_DIR, collection_name, os.path.splitext(filename)[0])
    
    if os.path.exists(file_img_path):
        for img_file in sorted(os.listdir(file_img_path)):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                if "formula" in img_file.lower() or "img_in_for" in img_file.lower():
                    continue
                full_path = os.path.join(file_img_path, img_file)
                images.append((full_path, img_file))
                
    return f"ğŸ“„ **{filename}**\n\n{summary}", images

def update_file_list(collection_name):
    ready, msg = check_ready()
    if not ready: return gr.update(choices=[], label="2. æ–‡æ¡£ (æœªè¿æ¥)")
    
    store = known_collections.get(collection_name, milvus_store)
    if not store: return gr.update(choices=[], label="2. æ–‡æ¡£ (åº“ä¸å­˜åœ¨)")
    
    files = store.list_documents()
    count = len(files)
    choices = ["å…¨éƒ¨æ–‡æ¡£ (Global QA)"] + files
    return gr.update(choices=choices, value=choices[0], label=f"2. æ–‡æ¡£ (å…± {count} ä¸ª)")

def update_file_list_for_delete(collection_name):
    ready, msg = check_ready()
    if not ready or not collection_name: 
        return gr.update(choices=[], label="é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶")
        
    store = known_collections.get(collection_name, milvus_store)
    files = store.list_documents()
    count = len(files)
    return gr.update(choices=files, value=None, label=f"é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶ (å½“å‰åº“å…± {count} ä¸ª)")

def run_recall_test(collection_name):
    ready, msg = check_ready()
    if not ready: return msg
    if not collection_name: return "âŒ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªçŸ¥è¯†åº“"

    store = known_collections.get(collection_name, milvus_store)
    return store.test_self_recall(sample_size=20)

def create_collection_ui(new_name):
    global ernie
    ready, msg = check_ready()
    if not ready: return gr.update(), msg
    if not new_name: return gr.update(), "âŒ åç§°ä¸èƒ½ä¸ºç©º"
    # try:
    #     # ä¼ å…¥å…¨å±€é…ç½®çš„ ernie
    #     new_store = MilvusVectorStore(
    #         uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"),
    #         collection_name=new_name, embedding_client=ernie
    #     )
    #     dummy = [{"filename":"_init","page":0,"content":"init","chunk_id":0}]
    #     new_store.insert_documents(dummy)
    #     known_collections[new_name] = new_store
    #     updated = list(known_collections.keys())
    #     return gr.update(choices=updated, value=new_name), f"âœ… åˆ›å»ºæˆåŠŸ: {new_name}"
    real_milvus_name = encode_name(new_name)
    
    try:
        # ğŸŸ¢ ä¼ ç»™ Milvus çš„æ˜¯ç¼–ç åçš„åå­—
        new_store = MilvusVectorStore(
            uri=os.environ.get("MILVUS_URI"), 
            token=os.environ.get("MILVUS_TOKEN"),
            collection_name=real_milvus_name, # <--- çœŸå®è¡¨å
            embedding_client=ernie
        )
        # # åˆå§‹åŒ–ä¸€ä¸‹
        # dummy = [{"filename":"_init","page":0,"content":"init","chunk_id":0}]
        # new_store.insert_documents(dummy)
        
        # ğŸŸ¢ å­—å…¸ Key ä¾ç„¶å­˜ä¸­æ–‡ new_nameï¼Œæ–¹ä¾¿ UI æ˜¾ç¤º
        known_collections[new_name] = new_store
        
        updated = list(known_collections.keys())
        return gr.update(choices=updated, value=new_name), f"âœ… åˆ›å»ºæˆåŠŸ: {new_name}"
    except Exception as e:
        return gr.update(), f"âŒ åˆ›å»ºå¤±è´¥: {e}"

def delete_single_file(collection_name, filename):
    ready, msg = check_ready()
    if not ready: return msg
    if not collection_name: return "âŒ è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“"
    if not filename: return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶"
    
    store = known_collections.get(collection_name, milvus_store)
    msg = store.delete_document(filename)
    
    try:
        img_dir = os.path.join(ASSET_DIR, collection_name, os.path.splitext(filename)[0])
        if os.path.exists(img_dir):
            shutil.rmtree(img_dir)
            msg += " (å…³è”å›¾ç‰‡å·²æ¸…ç†)"
    except: pass
    
    return msg

def delete_collection_ui(name):
    ready, msg = check_ready()
    if not ready: return gr.update(), msg
    if not name: return gr.update(), "è¯·é€‰æ‹©è¦åˆ é™¤çš„åº“"
    
    alias = "delete_conn"
    try:
        connections.connect(alias=alias, uri=os.environ.get("MILVUS_URI"), token=os.environ.get("MILVUS_TOKEN"))
        
        if utility.has_collection(name, using=alias): 
            utility.drop_collection(name, using=alias)
        
        if name in known_collections: 
            del known_collections[name]
        
        img_path = os.path.join(ASSET_DIR, name)
        if os.path.exists(img_path): shutil.rmtree(img_path)
        
        updated = list(known_collections.keys())
        val = updated[0] if updated else None
        return gr.update(choices=updated, value=val), f"ğŸ—‘ï¸ å·²åˆ é™¤: {name}"
    
    except Exception as e:
        return gr.update(), f"âŒ åˆ é™¤å¤±è´¥: {e}"
    
    finally:
        try:
            connections.disconnect(alias)
        except: pass

def refresh_all_dropdowns():
    if not system_ready: return gr.update(), gr.update(), gr.update()
    new_cols = scan_remote_collections()
    return (
        gr.update(choices=new_cols), 
        gr.update(choices=new_cols), 
        gr.update(choices=new_cols)
    )